# mcp/preprocessing.py\n\nimport statistics\nfrom collections import defaultdict\nimport time\n\n# --- Configuration Constants (adapt based on desired strictness) ---\n# Fees are often in msat/ppm, check LNBits data format\nMAX_CHANNEL_FEE_RATE_PPM = 20000 # Max allowed fee rate in ppm (Hydrus: 20k)\nMAX_CHANNEL_BASE_FEE_MSAT = 100000 # Max allowed base fee in msat (Hydrus: 100k)\n\ndef get_channel_age_blocks(channel_id: int) -> int:\n    """\n    Estimates the block height (age proxy) from the channel ID.\n    Assumes standard channel ID format (block_height:tx_index:output_index).\n\n    Args:\n        channel_id: The numeric channel ID (uint64).\n\n    Returns:\n        The estimated block height (int), or 0 if ID is invalid.\n    """\n    if not isinstance(channel_id, int) or channel_id <= 0:\n        return 0\n    # Extract the first 3 bytes (24 bits) for block height\n    block_height = channel_id >> 40\n    return block_height\n\ndef filter_channels(raw_edges_data: list) -> tuple[dict[str, list], int, int]:\n    """\n    Filters raw channel/edge data based on policies and validity.\n    Regroups valid channels by the public key of the node they belong to.\n\n    Args:\n        raw_edges_data: A list of dictionaries, each representing a channel edge\n                        from the graph source (e.g., LNBits). Expected keys per edge:\n                        \'channel_id\': int, \'chan_point\': str, \'capacity\': int (sats),\n                        \'node1_pub\': str, \'node2_pub\': str,\n                        \'node1_policy\': dict or None (with \'disabled\', \'fee_base_msat\', \'fee_rate_milli_msat\'),\n                        \'node2_policy\': dict or None (similar structure).\n\n    Returns:\n        A tuple containing:\n        - channels_by_node (dict[str, list]): A dictionary where keys are node pubkeys\n          and values are lists of processed channel dictionaries belonging to that node.\n          Channel dict structure: {\'channel_id\': int, \'chan_point\': str, \'peer_pubkey\': str,\n                                   \'capacity_sats\': int, \'age_blocks\': int, \'base_fee_msat\': int,\n                                   \'fee_rate_ppm\': int, \'min_htlc_msat\': int, \'max_htlc_msat\': int}\n        - total_valid_capacity (int): Sum of capacities of all valid, non-discarded channels.\n        - skipped_edges_count (int): Number of raw edges skipped due to missing policies or being discarded.\n    """\n    print(f"Filtering {len(raw_edges_data)} raw channel edges...")\n    start_time = time.time()\n    channels_by_node = defaultdict(list)\n    total_valid_capacity = 0\n    skipped_edges_count = 0\n    valid_edges_count = 0\n\n    for edge in raw_edges_data:\n        # Basic check for essential data\n        if not all(k in edge for k in [\'channel_id\', \'node1_pub\', \'node2_pub\', \'capacity\']):\n             skipped_edges_count += 1\n             continue\n\n        node1_policy = edge.get(\'node1_policy\')\n        node2_policy = edge.get(\'node2_policy\')\n        channel_id = edge[\'channel_id\']\n        capacity_sats = int(edge.get(\'capacity\', 0))\n        chan_point = edge.get(\'chan_point\', \'\')\n\n        # Skip channels where *both* policies are missing (incomplete propagation)\n        if node1_policy is None and node2_policy is None:\n            skipped_edges_count += 1\n            continue\n\n        # Estimate channel age\n        age_blocks = get_channel_age_blocks(channel_id)\n\n        # Check Node 1\'s perspective (policy for routing *from* Node 1)\n        policy1_discarded = False\n        if node1_policy:\n            if node1_policy.get(\'disabled\', False) or \\n               node1_policy.get(\'fee_rate_milli_msat\', 0) > MAX_CHANNEL_FEE_RATE_PPM or \\n               node1_policy.get(\'fee_base_msat\', 0) > MAX_CHANNEL_BASE_FEE_MSAT:\n                policy1_discarded = True\n        else:\n             policy1_discarded = True # Treat missing policy as unusable/discarded\n\n        if not policy1_discarded:\n            processed_channel = {\n                \'channel_id\': channel_id,\n                \'chan_point\': chan_point,\n                \'peer_pubkey\': edge[\'node2_pub\'],\n                \'capacity_sats\': capacity_sats,\n                \'age_blocks\': age_blocks,\n                \'base_fee_msat\': node1_policy.get(\'fee_base_msat\', 0),\n                \'fee_rate_ppm\': node1_policy.get(\'fee_rate_milli_msat\', 0), # Note: milli_msat is ppm\n                \'min_htlc_msat\': node1_policy.get(\'min_htlc\', 0), # Assuming key \'min_htlc\' from LND RPC structure\n                \'max_htlc_msat\': node1_policy.get(\'max_htlc_msat\', 0)\n                # Add inbound fees if available and needed\n            }\n            channels_by_node[edge[\'node1_pub\']].append(processed_channel)\n\n\n        # Check Node 2\'s perspective (policy for routing *from* Node 2)\n        policy2_discarded = False\n        if node2_policy:\n             if node2_policy.get(\'disabled\', False) or \\n                node2_policy.get(\'fee_rate_milli_msat\', 0) > MAX_CHANNEL_FEE_RATE_PPM or \\n                node2_policy.get(\'fee_base_msat\', 0) > MAX_CHANNEL_BASE_FEE_MSAT:\n                 policy2_discarded = True\n        else:\n              policy2_discarded = True # Treat missing policy as unusable/discarded\n\n        if not policy2_discarded:\n             processed_channel = {\n                 \'channel_id\': channel_id,\n                 \'chan_point\': chan_point,\n                 \'peer_pubkey\': edge[\'node1_pub\'],\n                 \'capacity_sats\': capacity_sats,\n                 \'age_blocks\': age_blocks,\n                 \'base_fee_msat\': node2_policy.get(\'fee_base_msat\', 0),\n                 \'fee_rate_ppm\': node2_policy.get(\'fee_rate_milli_msat\', 0),\n                 \'min_htlc_msat\': node2_policy.get(\'min_htlc\', 0),\n                 \'max_htlc_msat\': node2_policy.get(\'max_htlc_msat\', 0)\n                 # Add inbound fees if available and needed\n             }\n             channels_by_node[edge[\'node2_pub\']].append(processed_channel)\n\n        # If at least one policy was valid, count capacity and valid edge\n        if not policy1_discarded or not policy2_discarded:\n             total_valid_capacity += capacity_sats\n             valid_edges_count += 1\n        else:\n             # Both policies were invalid/discarded\n             skipped_edges_count += 1\n\n    end_time = time.time()\n    print(f"Channel filtering finished in {end_time - start_time:.2f}s.")\n    print(f"  Total valid edges considered: {valid_edges_count}")\n    print(f"  Total capacity from valid edges: {total_valid_capacity} sats")\n    print(f"  Edges skipped (incomplete/discarded policies): {skipped_edges_count}")\n\n    # Sanity check like in hydrus\n    if len(raw_edges_data) > 0 and skipped_edges_count > len(raw_edges_data) / 2:\n         print(f"Warning: More than half ({skipped_edges_count}/{len(raw_edges_data)}) of raw edges were skipped. Graph data might be incomplete.")\n         # raise ValueError("Graph data too incomplete to proceed reliably.") # Optional: raise error\n\n    return dict(channels_by_node), total_valid_capacity, valid_edges_count\n\n\ndef pre_filter_nodes(raw_nodes_data: list, channels_by_node: dict[str, list], total_valid_capacity: int, total_valid_edges: int) -> list:\n    """\n    Performs initial filtering on the list of raw node data based on basic criteria.\n\n    Args:\n        raw_nodes_data: List of dictionaries, each representing a node from the source.\n                        Expected keys: \'pub_key\', \'alias\', \'addresses\': list[str], \'features\': dict.\n        channels_by_node: Dictionary mapping node pubkeys to their list of *valid* channels\n                          (output from filter_channels).\n        total_valid_capacity: Total capacity sum from valid channels.\n        total_valid_edges: Total count of valid, non-discarded channel edges.\n\n    Returns:\n        A list of node dictionaries that passed the pre-filtering. Each dictionary\n        is enriched with \'channels\' (list of valid channels), \'capacity_sats\', and\n        \'features_count\'.\n    """\n    if not raw_nodes_data:\n        return []\n\n    print(f"Pre-filtering {len(raw_nodes_data)} raw nodes...")\n    start_time = time.time()\n    filtered_nodes = []\n    num_initial_nodes = len(raw_nodes_data)\n    skipped_no_address = 0\n    skipped_low_capacity = 0\n    skipped_low_channels = 0\n\n    # Calculate averages (avoid division by zero)\n    avg_node_capacity = (total_valid_capacity / num_initial_nodes) if num_initial_nodes > 0 else 0\n    # Average channels per node *perspective* (edge has two perspectives)\n    avg_node_channels = (total_valid_edges * 2 / num_initial_nodes) if num_initial_nodes > 0 else 0\n    print(f"  Average Node Capacity (approx): {avg_node_capacity:.0f} sats")\n    print(f"  Average Node Channels (approx): {avg_node_channels:.1f}")\n\n    processed_nodes = 0\n    for node in raw_nodes_data:\n        processed_nodes += 1\n        if processed_nodes % 5000 == 0: # Print progress less often for nodes\n             print(f"  Processed {processed_nodes}/{num_initial_nodes} nodes...")\n\n        pubkey = node.get(\'pub_key\') # Assuming \'pub_key\' from LND RPC structure\n        if not pubkey: continue # Skip nodes without pubkey\n\n        addresses = node.get(\'addresses\', [])\n        node_valid_channels = channels_by_node.get(pubkey, [])\n        num_valid_channels = len(node_valid_channels)\n\n        # 1. Filter: No addresses\n        # Check if addresses list is present and contains at least one non-empty string\n        if not addresses or not any(addr for addr in addresses if isinstance(addr, str)):\n            skipped_no_address += 1\n            continue\n\n        # Calculate node\'s total capacity from its valid channels\n        node_capacity_sats = sum(c.get(\'capacity_sats\', 0) for c in node_valid_channels)\n\n        # 2. Filter: Capacity below average\n        if node_capacity_sats < avg_node_capacity:\n            skipped_low_capacity += 1\n            continue\n\n        # 3. Filter: Number of valid channels below average\n        if num_valid_channels < avg_node_channels:\n            skipped_low_channels += 1\n            continue\n\n        # --- Node passed pre-filtering ---\n\n        # Extract features count (simple count like hydrus)\n        features_map = node.get(\'features\', {})\n        features_count = 0\n        if isinstance(features_map, dict): # Ensure features is a dict\n             features_count = sum(1 for f in features_map.values() if isinstance(f, dict) and f.get(\'is_known\', False))\n\n        # Enrich node data for next steps\n        processed_node_data = {\n            \'pubkey\': pubkey,\n            \'alias\': node.get(\'alias\', \'\'),\n            \'addresses\': addresses,\n            \'features_count\': features_count,\n            \'capacity_sats\': node_capacity_sats, # Total capacity from *valid* channels\n            \'channels\': node_valid_channels # List of *valid* channels from this node\'s perspective\n            # Centrality will be added later\n        }\n        filtered_nodes.append(processed_node_data)\n\n    end_time = time.time()\n    print(f"Node pre-filtering finished in {end_time - start_time:.2f}s.")\n    print(f"  Nodes remaining after pre-filtering: {len(filtered_nodes)}/{num_initial_nodes}")\n    print(f"  Skipped (no address): {skipped_no_address}")\n    print(f"  Skipped (low capacity): {skipped_low_capacity}")\n    print(f"  Skipped (low channels): {skipped_low_channels}")\n\n    return filtered_nodes 