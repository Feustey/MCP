# mcp/graph_analysis.py\n\nimport networkx as nx\nimport time\n\ndef build_graph(filtered_nodes_data: list) -> nx.Graph:\n    """\n    Builds a networkx graph from the pre-filtered list of node data.\n\n    Args:\n        filtered_nodes_data: A list of dictionaries, where each dictionary\n                             represents a node that has passed initial filtering.\n                             Each node dict should have at least \'pubkey\' and\n                             \'channels\': [{'peer_pubkey': str, ...}, ...].\n\n    Returns:\n        A networkx Graph object representing the connections between filtered nodes.\n        Nodes are identified by their public keys.\n    """\n    G = nx.Graph()\n    if not filtered_nodes_data:\n        return G # Return empty graph if no nodes\n\n    print(f"Building networkx graph from {len(filtered_nodes_data)} filtered nodes...")\n    start_time = time.time()\n\n    node_pubkeys = {node[\'pubkey\'] for node in filtered_nodes_data if \'pubkey\' in node}\n    G.add_nodes_from(node_pubkeys)\n\n    edges_added = 0\n    for node_data in filtered_nodes_data:\n        node_pubkey = node_data.get(\'pubkey\')\n        if not node_pubkey: continue # Skip if no pubkey\n\n        channels = node_data.get(\'channels\', [])\n        for channel in channels:\n            peer_pubkey = channel.get(\'peer_pubkey\')\n            # Add edge only if the peer is also in our filtered node set\n            # and the edge hasn\'t been added already (nx.Graph handles duplicates)\n            if peer_pubkey and peer_pubkey in node_pubkeys:\n                 # Optionally add channel capacity as weight if needed later\n                 # capacity = channel.get(\'capacity_sats\', 1) # Default weight 1 if none\n                 # G.add_edge(node_pubkey, peer_pubkey, capacity=capacity)\n                 G.add_edge(node_pubkey, peer_pubkey)\n                 edges_added += 1 # Count logical edges considered\n\n    end_time = time.time()\n    # Note: G.number_of_edges() counts unique edges (undirected)\n    print(f"Graph built in {end_time - start_time:.2f}s. Nodes: {G.number_of_nodes()}, Edges: {G.number_of_edges()}")\n    return G\n\ndef calculate_centralities(G: nx.Graph) -> dict:\n    """\n    Calculates various centrality measures for all nodes in the graph.\n\n    Args:\n        G: The networkx Graph object.\n\n    Returns:\n        A dictionary where keys are node public keys and values are another\n        dictionary containing the calculated centrality metrics:\n        {\n            \'node_pubkey\': {\n                \'degree\': float, \n                \'betweenness\': float,\n                \'closeness\': float, \n                \'eigenvector\': float\n            }, ...\n        }\n        Returns an empty dict if the graph is empty.\n    """\n    centrality_results = {}\n    if not G or G.number_of_nodes() == 0:\n        print("Cannot calculate centralities: Graph is empty.")\n        return centrality_results\n\n    print("Calculating centralities...")\n    start_time = time.time()\n\n    # 1. Degree Centrality (Normalized by N-1 where N is number of nodes)\n    # networkx.degree_centrality gives degree/(N-1)\n    print("  Calculating degree centrality...")\n    degree_centrality = nx.degree_centrality(G)\n    # Hydrus normalized by total number of *channels*. Let\'s stick to networkx default (N-1) for now,\n    # as it\'s standard. We can adapt if needed.\n\n    # 2. Betweenness Centrality\n    # k=None uses all nodes, can be slow. Consider sampling (k=...) for large graphs.\n    print("  Calculating betweenness centrality (this may take time)...")\n    # Using approximate betweenness for potentially large graphs (k=number_of_nodes * some_fraction)\n    # Set k to None for exact calculation if performance allows\n    k_betweenness = None # Set to e.g., int(G.number_of_nodes() * 0.1) for approx on large graphs\n    if k_betweenness and k_betweenness >= G.number_of_nodes():\n         k_betweenness = None # Ensure k is smaller than N if specified\n    betweenness_centrality = nx.betweenness_centrality(G, k=k_betweenness, normalized=True)\n\n    # 3. Closeness Centrality\n    # Calculates (N-1) / sum_of_distances for reachable nodes in each component.\n    # Note: For disconnected graphs, closeness is calculated per component.\n    # Nodes unreachable from a node 'u' are not considered in the average path length for 'u'.\n    print("  Calculating closeness centrality...")\n    closeness_centrality = nx.closeness_centrality(G)\n\n    # 4. Eigenvector Centrality\n    # Can fail on graphs with multiple components. max_iter increase might be needed.\n    # tol = tolerance for convergence\n    print("  Calculating eigenvector centrality...")\n    try:\n        # Increase max_iter for potentially better convergence on complex graphs\n        eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000, tol=1.0e-6)\n    except nx.PowerIterationFailedConvergence:\n        print("  Warning: Eigenvector centrality did not converge with increased max_iter. Results might be approximate.")\n        # Attempt with default parameters as fallback\n        try:\n            eigenvector_centrality = nx.eigenvector_centrality(G, tol=1.0e-4) # Try slightly looser default tolerance\n        except Exception as e:\n             print(f"  Error: Eigenvector centrality failed definitively: {e}. Assigning 0.")\n             # Assign 0 as a fallback if it completely fails\n             eigenvector_centrality = {node: 0.0 for node in G.nodes()}\n    except Exception as e: # Catch other potential errors\n        print(f"  Error calculating Eigenvector centrality: {e}. Assigning 0.")\n        eigenvector_centrality = {node: 0.0 for node in G.nodes()}\n\n\n    end_time = time.time()\n    print(f"Centralities calculated in {end_time - start_time:.2f}s.")\n\n    # Combine results into the desired output format\n    for node in G.nodes():\n        centrality_results[node] = {\n            \'degree\': degree_centrality.get(node, 0.0),\n            \'betweenness\': betweenness_centrality.get(node, 0.0),\n            \'closeness\': closeness_centrality.get(node, 0.0),\n            \'eigenvector\': eigenvector_centrality.get(node, 0.0)\n        }\n\n    return centrality_results\n\n# Example Usage (for testing, would be called by the API handler)\nif __name__ == \'__main__\':\n    # Sample data mimicking filtered_nodes_data structure\n    sample_nodes = [\n        {\'pubkey\': \'A\', \'channels\': [{\'peer_pubkey\': \'B\'}, {\'peer_pubkey\': \'C\'}]},\n        {\'pubkey\': \'B\', \'channels\': [{\'peer_pubkey\': \'A\'}, {\'peer_pubkey\': \'C\'}, {\'peer_pubkey\': \'D\'}]},\n        {\'pubkey\': \'C\', \'channels\': [{\'peer_pubkey\': \'A\'}, {\'peer_pubkey\': \'B\'}, {\'peer_pubkey\': \'D\'}]},\n        {\'pubkey\': \'D\', \'channels\': [{\'peer_pubkey\': \'B\'}, {\'peer_pubkey\': \'C\'}, {\'peer_pubkey\': \'E\'}]},\n        {\'pubkey\': \'E\', \'channels\': [{\'peer_pubkey\': \'D\'}]},\n        {\'pubkey\': \'F\', \'channels\': []} # Isolated node\n    ]\n\n    graph = build_graph(sample_nodes)\n\n    if graph.number_of_nodes() > 0:\n        centralities = calculate_centralities(graph)\n        print("\nCalculated Centralities:")\n        for node, metrics in centralities.items():\n            print(f"  Node {node}:")\n            print(f"    Degree:      {metrics[\'degree\']:.4f}")\n            print(f"    Betweenness: {metrics[\'betweenness\']:.4f}")\n            print(f"    Closeness:   {metrics[\'closeness\']:.4f}")\n            print(f"    Eigenvector: {metrics[\'eigenvector\']:.4f}")\n\n 