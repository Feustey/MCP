# Sp√©cifications MCP v1.0 - Roadmap vers Production Stable
> Derni√®re mise √† jour: 12 octobre 2025
> Version: 1.0.0-prod
> Auteur: √âquipe MCP

---

## üìã TABLE DES MATI√àRES

1. [Vue d'ensemble](#vue-densemble)
2. [Priorit√© 1 - Infrastructure Stable](#priorit√©-1---infrastructure-stable)
3. [Priorit√© 2 - Core Engine Complet](#priorit√©-2---core-engine-complet)
4. [Priorit√© 3 - Production Contr√¥l√©e](#priorit√©-3---production-contr√¥l√©e)
5. [Priorit√© 4 - Fonctionnalit√©s Avanc√©es](#priorit√©-4---fonctionnalit√©s-avanc√©es)
6. [Timeline & Ressources](#timeline--ressources)
7. [Crit√®res de Succ√®s](#crit√®res-de-succ√®s)
8. [Annexes](#annexes)

---

## VUE D'ENSEMBLE

### Contexte

MCP (Model Context Protocol) est un syst√®me d'optimisation autonome pour n≈ìuds Lightning Network. Suite √† la r√©solution des 828 failures et √† la stabilisation de l'API en production (octobre 2025), cette sp√©cification d√©finit la roadmap vers une version 1.0 production-ready.

### P√©rim√®tre V1.0

**Inclus** :
- Infrastructure cloud-native stable
- Core Engine d'optimisation de fees complet
- Int√©grations Lightning r√©elles (LNBits, LND)
- Mode Shadow valid√© et production contr√¥l√©e
- Monitoring et observabilit√© complets
- APIs et services avanc√©s (RAG, Scoring)

**Exclu (report√© en V2)** :
- Packaging Umbrel
- Installation locale-first
- Distribution app store Umbrel

### Objectifs Principaux

1. **Stabilit√©** : 99% uptime, 0 failures critiques
2. **Fonctionnalit√©** : Core Engine 100% op√©rationnel avec LND/LNBits r√©els
3. **Performance** : < 500ms response time, cache optimis√©
4. **Observabilit√©** : Monitoring complet, m√©triques temps r√©el
5. **S√©curit√©** : Authentification, chiffrement, isolation donn√©es

---

## PRIORIT√â 1 - INFRASTRUCTURE STABLE

> **Dur√©e estim√©e** : 1-2 semaines  
> **Criticit√©** : üî¥ CRITIQUE  
> **D√©pendances** : Aucune

### 1.1 Configuration Serveur Production

#### Objectif
Finaliser la configuration de l'infrastructure serveur pour un d√©ploiement stable et automatis√©.

#### T√¢ches

**1.1.1 Configuration Nginx avec HTTPS**

```yaml
Responsable: DevOps
Dur√©e: 2 heures
Pr√©requis: Acc√®s sudo sur serveur production

Actions:
  - Ex√©cuter script configure_nginx_production.sh
  - Configurer reverse proxy port 80/443 ‚Üí 8000
  - Installer certificat SSL Let's Encrypt
  - Tester acc√®s https://api.dazno.de/

Crit√®res de succ√®s:
  - ‚úÖ API accessible via HTTPS
  - ‚úÖ Certificat SSL valide (A+ SSL Labs)
  - ‚úÖ Redirection HTTP ‚Üí HTTPS automatique
  - ‚úÖ Headers s√©curit√© (HSTS, CSP, etc.)

Fichiers:
  - scripts/configure_nginx_production.sh
  - nginx-simple.conf (template existant)
```

**1.1.2 Service Systemd avec Auto-restart**

```yaml
Responsable: DevOps
Dur√©e: 1 heure
Pr√©requis: Configuration nginx termin√©e

Actions:
  - Ex√©cuter script configure_systemd_autostart.sh
  - Cr√©er service mcp-api.service
  - Activer auto-start au boot
  - Configurer restart automatique sur crash

Crit√®res de succ√®s:
  - ‚úÖ Service d√©marre automatiquement au boot
  - ‚úÖ Restart automatique en cas de crash (< 10s)
  - ‚úÖ Logs systemd accessibles (journalctl)
  - ‚úÖ Status v√©rifiable (systemctl status mcp-api)

Fichiers:
  - scripts/configure_systemd_autostart.sh
  - /etc/systemd/system/mcp-api.service
  - /home/feustey/mcp-production/start_api.sh
```

**1.1.3 Monitoring Infrastructure**

```yaml
Responsable: DevOps
Dur√©e: 3 heures
Pr√©requis: Service systemd configur√©

Actions:
  - Adapter monitoring pour endpoint / au lieu de /health
  - Configurer alertes Telegram pour service down
  - Impl√©menter healthcheck avanc√© systemd
  - Logs rotation et archivage

Crit√®res de succ√®s:
  - ‚úÖ Monitoring d√©tecte correctement l'√©tat API
  - ‚úÖ Alertes envoy√©es en < 2 min si service down
  - ‚úÖ Logs rotationn√©s quotidiennement
  - ‚úÖ Historique monitoring persistant (> 30 jours)

Fichiers:
  - monitor_production.py (modifier endpoint)
  - logs/ (configuration rotation)
```

### 1.2 Reconstruction Image Docker

#### Objectif
Cr√©er une image Docker stable et optimis√©e pour remplacer l'image d√©fectueuse actuelle.

#### Probl√®mes Actuels
- ‚ùå Image `feustey/mcp-dazno:latest` crashloop
- ‚ùå Entrypoint cass√©
- ‚ùå D√©pendances manquantes (pandas, numpy)
- ‚ùå Structure modules incorrecte

#### T√¢ches

**1.2.1 Audit et Nettoyage Dockerfile**

```yaml
Responsable: Backend Dev
Dur√©e: 1 jour
Pr√©requis: Acc√®s au code source

Actions:
  - Auditer Dockerfile existant
  - Identifier d√©pendances manquantes
  - V√©rifier structure PYTHONPATH
  - Cr√©er Dockerfile.production propre

Crit√®res de succ√®s:
  - ‚úÖ Build local r√©ussi
  - ‚úÖ Container d√©marre sans erreur
  - ‚úÖ Healthcheck int√©gr√© fonctionnel
  - ‚úÖ Taille image < 1GB

D√©pendances √† inclure:
  fastapi>=0.104.0
  uvicorn[standard]>=0.24.0
  pydantic>=2.5.0
  pydantic-settings>=2.1.0
  httpx>=0.25.0
  pandas>=2.1.0
  numpy>=1.24.0
  redis>=5.0.0
  pymongo>=4.5.0
  anthropic>=0.7.0
  qdrant-client>=1.7.0
  structlog>=23.2.0
  
Fichiers:
  - Dockerfile.production (nouveau)
  - requirements-production.txt (minimal)
  - docker_entrypoint.sh (corrig√©)
```

**1.2.2 Build et Tests Image**

```yaml
Responsable: Backend Dev
Dur√©e: 1 jour
Pr√©requis: Dockerfile.production cr√©√©

Actions:
  - Builder image localement
  - Tests unitaires dans container
  - Tests d'int√©gration complets
  - Push vers registry (DockerHub ou GCP)

Crit√®res de succ√®s:
  - ‚úÖ Image build en < 10 min
  - ‚úÖ Tous tests passent (100%)
  - ‚úÖ Healthcheck respond < 1s
  - ‚úÖ Memory footprint < 500MB

Commandes:
  docker build -t mcp-api:1.0.0 -f Dockerfile.production .
  docker run --rm mcp-api:1.0.0 pytest tests/
  docker push registry.example.com/mcp-api:1.0.0

Fichiers:
  - Dockerfile.production
  - docker-compose.production.yml (mise √† jour)
```

**1.2.3 D√©ploiement Image Production**

```yaml
Responsable: DevOps
Dur√©e: 2 heures
Pr√©requis: Image test√©e et push√©e

Actions:
  - Mettre √† jour docker-compose.production.yml
  - D√©ployer nouvelle image sur serveur
  - Migration sans downtime (blue/green)
  - Validation post-d√©ploiement

Crit√®res de succ√®s:
  - ‚úÖ D√©ploiement sans downtime
  - ‚úÖ Rollback possible en < 1 min
  - ‚úÖ Logs propres (no errors)
  - ‚úÖ Monitoring confirme healthy

Fichiers:
  - docker-compose.production.yml
  - scripts/deploy_docker_production.sh (nouveau)
```

### 1.3 Activation Services Cloud

#### Objectif
Connecter les services cloud r√©els (MongoDB, Redis) pour sortir du mode d√©grad√©.

#### T√¢ches

**1.3.1 Configuration MongoDB Atlas**

```yaml
Responsable: Backend Dev
Dur√©e: 4 heures
Pr√©requis: Compte MongoDB Atlas provisionn√©

Actions:
  - Cr√©er cluster production MongoDB Atlas
  - Configurer network access et whitelisting
  - G√©n√©rer connection string s√©curis√©e
  - Cr√©er collections et indexes

Crit√®res de succ√®s:
  - ‚úÖ Connexion stable (< 50ms latency)
  - ‚úÖ Indexes optimis√©s cr√©√©s
  - ‚úÖ Backup automatique configur√© (daily)
  - ‚úÖ Monitoring Atlas actif

Configuration:
  Tier: M10 (Production, 2GB RAM)
  Region: eu-west-1 (Frankfurt)
  Backup: Daily snapshots, 7 jours retention
  
Collections:
  - nodes (index: node_id, created_at)
  - channels (index: channel_id, node_id, created_at)
  - policies (index: channel_id, applied_at)
  - metrics (index: node_id, timestamp)
  - decisions (index: node_id, decision_type, created_at)

Variables .env:
  MONGODB_URL=mongodb+srv://user:pass@cluster.mongodb.net/mcp_prod
  MONGODB_DATABASE=mcp_prod
  MONGODB_CONNECTION_POOL_SIZE=50
  MONGODB_TIMEOUT_MS=5000
```

**1.3.2 Configuration Redis Cloud**

```yaml
Responsable: Backend Dev
Dur√©e: 3 heures
Pr√©requis: Compte Redis Cloud ou Upstash

Actions:
  - Provisionner instance Redis Cloud
  - Configurer TLS et authentification
  - Impl√©menter cache layer dans app
  - Tests de performance cache

Crit√®res de succ√®s:
  - ‚úÖ Latency < 10ms (p95)
  - ‚úÖ Hit rate > 80% apr√®s warm-up
  - ‚úÖ TTL configur√©s par type de donn√©e
  - ‚úÖ Eviction policy configur√©e (LRU)

Configuration:
  Provider: Redis Cloud / Upstash
  Tier: 250MB RAM
  Region: eu-west-1
  TLS: Enabled
  
Cache Strategy:
  - Node data: TTL 5 min
  - Channel data: TTL 10 min
  - Metrics: TTL 1 min
  - Scores: TTL 15 min
  - Heavy queries: TTL 30 min

Variables .env:
  REDIS_URL=rediss://default:pass@redis-cluster.cloud.redislabs.com:6379
  REDIS_MAX_CONNECTIONS=50
  REDIS_TIMEOUT=5
```

**1.3.3 Gestion Mode D√©grad√©**

```yaml
Responsable: Backend Dev
Dur√©e: 1 jour
Pr√©requis: MongoDB et Redis configur√©s

Actions:
  - Impl√©menter fallback gracieux
  - Circuit breaker pour services externes
  - Mode d√©grad√© si service indisponible
  - Tests de r√©silience

Crit√®res de succ√®s:
  - ‚úÖ API fonctionne si Redis down
  - ‚úÖ API fonctionne si MongoDB down
  - ‚úÖ D√©gradation progressive, pas de crash
  - ‚úÖ Alertes envoy√©es si mode d√©grad√©

Impl√©mentation:
  - Circuit breaker: 5 failures ‚Üí open (30s)
  - Fallback MongoDB: Logs en fichier
  - Fallback Redis: No cache (direct queries)
  - Health endpoint refl√®te l'√©tat d√©grad√©

Fichiers:
  - src/tools/circuit_breaker.py (existant)
  - app/services/fallback_manager.py (nouveau)
```

---

## PRIORIT√â 2 - CORE ENGINE COMPLET

> **Dur√©e estim√©e** : 2-3 semaines  
> **Criticit√©** : üî¥ CRITIQUE  
> **D√©pendances** : Priorit√© 1 termin√©e

### 2.1 Int√©gration LNBits R√©elle

#### Objectif
Remplacer les mocks par une int√©gration compl√®te avec LNBits pour l'ex√©cution r√©elle des policies de fees.

#### T√¢ches

**2.1.1 Client LNBits Complet**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: Acc√®s LNBits instance production

Actions:
  - Finaliser src/clients/lnbits_client.py
  - Impl√©menter tous les endpoints n√©cessaires
  - Authentification macaroon compl√®te
  - Gestion erreurs et retry logic

Endpoints requis:
  GET /api/v1/wallet
  GET /api/v1/payments
  POST /api/v1/payments
  GET /lightning/api/v1/channels
  POST /lightning/api/v1/channel_policy
  GET /lightning/api/v1/node_info

Crit√®res de succ√®s:
  - ‚úÖ Tous endpoints impl√©ment√©s et test√©s
  - ‚úÖ Authentification macaroon fonctionnelle
  - ‚úÖ Rate limiting respect√© (100 req/min)
  - ‚úÖ Retry automatique (3x avec backoff)
  - ‚úÖ Timeouts configurables
  - ‚úÖ Tests unitaires > 90% coverage

Fichiers:
  - src/clients/lnbits_client.py (compl√©ter)
  - tests/test_lnbits_client.py (nouveau)
  - config.py (ajouter LNBits config)
```

**2.1.2 Authentification Macaroon**

```yaml
Responsable: Backend Dev
Dur√©e: 2 jours
Pr√©requis: Client LNBits impl√©ment√©

Actions:
  - Impl√©menter g√©n√©ration macaroon
  - Gestion s√©curis√©e des credentials
  - Rotation automatique des tokens
  - Validation et refresh

Crit√®res de succ√®s:
  - ‚úÖ Macaroons stock√©s chiffr√©s
  - ‚úÖ Rotation automatique (30 jours)
  - ‚úÖ R√©vocation possible
  - ‚úÖ Logs audit des acc√®s

S√©curit√©:
  - Chiffrement: AES-256-GCM
  - Storage: MongoDB (encrypted field)
  - Rotation: Automatique tous les 30j
  - Permissions: Read-only par d√©faut

Variables .env:
  LNBITS_URL=https://lnbits.example.com
  LNBITS_ADMIN_KEY=encrypted_key_here
  LNBITS_INVOICE_KEY=encrypted_key_here
  MACAROON_ENCRYPTION_KEY=32_bytes_random_key
  MACAROON_ROTATION_DAYS=30

Fichiers:
  - src/auth/macaroon_manager.py (nouveau)
  - src/auth/encryption.py (nouveau)
```

**2.1.3 Ex√©cution Policies R√©elles**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: LNBits client et auth fonctionnels

Actions:
  - Compl√©ter src/tools/optimize_and_execute.py
  - Impl√©menter apply_policy() r√©el
  - Validation avant application
  - Rollback automatique si √©chec

Workflow:
  1. G√©n√©ration recommandation par optimizer
  2. Validation r√®gles business
  3. Dry-run simulation
  4. Backup policy actuelle
  5. Application via LNBits API
  6. V√©rification post-application
  7. Rollback si √©chec

Crit√®res de succ√®s:
  - ‚úÖ Application policy r√©ussie (100%)
  - ‚úÖ Backup automatique avant chaque change
  - ‚úÖ Rollback fonctionnel en < 30s
  - ‚úÖ Logs d√©taill√©s de chaque action
  - ‚úÖ Notifications Telegram pour chaque change

S√©curit√©:
  - Mode dry-run par d√©faut
  - Confirmation manuelle pour prod (v1.0)
  - Limites: max 5 changes/jour par canal
  - Blacklist canaux critiques

Fichiers:
  - src/tools/optimize_and_execute.py (compl√©ter)
  - src/optimizers/policy_validator.py (nouveau)
  - src/tools/rollback_manager.py (nouveau)
```

### 2.2 Core Fee Optimizer

#### Objectif
Finaliser le moteur d'optimisation des fees avec heuristiques compl√®tes et production-ready.

#### T√¢ches

**2.2.1 Heuristiques Avanc√©es**

```yaml
Responsable: Backend Dev + Data Analyst
Dur√©e: 1 semaine
Pr√©requis: Donn√©es r√©elles disponibles

Actions:
  - Impl√©menter toutes les heuristiques d√©finies
  - Calibration des poids sur donn√©es historiques
  - Tests A/B sur simulations
  - Documentation algorithmes

Heuristiques impl√©ment√©es:
  1. Centrality Score (betweenness, closeness)
  2. Liquidity Balance (local/remote ratio)
  3. Forward Activity (success rate, volume)
  4. Fee Competitiveness (vs network median)
  5. Uptime & Reliability
  6. Age & Stability
  7. Peer Quality Score
  8. Network Position (hub vs edge)

Pond√©rations par d√©faut:
  centrality: 0.20
  liquidity: 0.25
  activity: 0.20
  competitiveness: 0.15
  reliability: 0.10
  age: 0.05
  peer_quality: 0.03
  position: 0.02

Crit√®res de succ√®s:
  - ‚úÖ Toutes heuristiques impl√©ment√©es
  - ‚úÖ Tests unitaires > 95% coverage
  - ‚úÖ Calibration sur > 1000 canaux
  - ‚úÖ Performance < 100ms par canal
  - ‚úÖ Documentation compl√®te algorithmes

Fichiers:
  - src/optimizers/core_fee_optimizer.py (am√©liorer)
  - src/optimizers/heuristics/ (nouveau dossier)
    - centrality.py
    - liquidity.py
    - activity.py
    - competitiveness.py
    - reliability.py
  - docs/heuristics-specification.md (nouveau)
```

**2.2.2 Decision Engine**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: Heuristiques impl√©ment√©es

Actions:
  - Fonction pure evaluate_channel()
  - R√®gles de d√©cision claires
  - Thresholds configurables
  - Logs explicites pour chaque d√©cision

Types de d√©cisions:
  NO_ACTION: Score optimal (0.7-1.0)
  INCREASE_FEES: Sous-utilis√© (score < 0.3)
  DECREASE_FEES: Sur-pric√© (score 0.3-0.5, low activity)
  REBALANCE: D√©s√©quilibr√© (local/remote > 0.8 ou < 0.2)
  CLOSE_CHANNEL: Mort (score < 0.1, no activity 30d)

Thresholds configurables:
  optimal_min: 0.7
  increase_threshold: 0.3
  decrease_threshold: 0.5
  rebalance_ratio_max: 0.8
  rebalance_ratio_min: 0.2
  close_threshold: 0.1
  close_inactivity_days: 30

Crit√®res de succ√®s:
  - ‚úÖ Fonction pure, d√©terministe
  - ‚úÖ Logs explicites (pourquoi cette d√©cision)
  - ‚úÖ Tests avec cas limites
  - ‚úÖ Performance < 50ms

Fichiers:
  - src/optimizers/decision_engine.py (nouveau)
  - config/decision_thresholds.yaml (nouveau)
  - tests/test_decision_engine.py (nouveau)
```

**2.2.3 Syst√®me de Rollback Transactionnel**

```yaml
Responsable: Backend Dev
Dur√©e: 2 jours
Pr√©requis: Decision engine impl√©ment√©

Actions:
  - Backup automatique avant action
  - Transaction log d√©taill√©
  - Rollback manuel et automatique
  - Tests de recovery

Workflow backup:
  1. Snapshot policy actuelle
  2. Store dans MongoDB (collection: policy_backups)
  3. R√©f√©rence dans decision log
  4. Retention: 90 jours

Rollback automatique si:
  - √âchec application policy
  - Error rate spike (> 50% en 5 min)
  - Latency spike (> 2x normale)
  - Manual trigger via API

Crit√®res de succ√®s:
  - ‚úÖ Backup avant chaque change (100%)
  - ‚úÖ Rollback r√©ussi < 30s
  - ‚úÖ Tests de recovery passent
  - ‚úÖ Historique complet tra√ßable

Fichiers:
  - src/tools/rollback_manager.py (am√©liorer)
  - src/tools/transaction_log.py (nouveau)
```

### 2.3 Lightning Scoring Service

#### Objectif
Activer le syst√®me de scoring multicrit√®re pour les n≈ìuds et canaux du r√©seau Lightning.

#### T√¢ches

**2.3.1 Int√©gration Service de Scoring**

```yaml
Responsable: Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Core optimizer finalis√©

Actions:
  - Activer app/services/lightning_scoring.py
  - Endpoints API /api/v1/lightning/scores/
  - Calculs centrality, reliability, performance
  - Cache r√©sultats scoring

Endpoints:
  GET /api/v1/lightning/scores/node/{node_id}
  GET /api/v1/lightning/scores/channel/{channel_id}
  POST /api/v1/lightning/scores/batch
  GET /api/v1/lightning/scores/rankings

M√©triques calcul√©es:
  - Node centrality (betweenness, closeness, eigenvector)
  - Node reliability (uptime, success rate, reputation)
  - Channel performance (forward success, fees earned, volume)
  - Channel health (balance, age, activity)

Crit√®res de succ√®s:
  - ‚úÖ Tous endpoints fonctionnels
  - ‚úÖ Cache Redis hit rate > 80%
  - ‚úÖ Response time < 200ms (cached)
  - ‚úÖ Response time < 2s (uncached)
  - ‚úÖ Tests unitaires > 90% coverage

Fichiers:
  - app/services/lightning_scoring.py (activer)
  - app/routes/lightning_scoring.py (nouveau)
  - tests/test_lightning_scoring.py (nouveau)
```

**2.3.2 Int√©gration Donn√©es R√©seau**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: Scoring service actif

Actions:
  - Sync graph Lightning Network
  - Calculs topologie (NetworkX)
  - Update p√©riodique (cron 6h)
  - M√©triques r√©seau global

Sources de donn√©es:
  - Lightning Network Graph (via LND gossip)
  - Amboss API (node stats)
  - 1ML API (rankings)
  - Mempool.space (on-chain data)

M√©triques r√©seau calcul√©es:
  - Total nodes, total channels
  - Network capacity, median channel size
  - Average node degree, clustering coefficient
  - Top hubs (centrality rankings)

Crit√®res de succ√®s:
  - ‚úÖ Graph sync < 5 min
  - ‚úÖ Calculs topologie < 10 min
  - ‚úÖ Update automatique 4x/jour
  - ‚úÖ M√©triques persist√©es MongoDB

Fichiers:
  - src/scanners/network_graph_scanner.py (nouveau)
  - src/analysis/network_topology.py (nouveau)
  - scripts/sync_network_graph.py (nouveau)
```

---

## PRIORIT√â 3 - PRODUCTION CONTR√îL√âE

> **Dur√©e estim√©e** : 3-4 semaines  
> **Criticit√©** : üü° HAUTE  
> **D√©pendances** : Priorit√© 1 & 2 termin√©es

### 3.1 Shadow Mode Extended

#### Objectif
Observer le syst√®me en mode "observation only" pendant 14-21 jours minimum avant activation r√©elle.

#### T√¢ches

**3.1.1 Configuration Shadow Mode**

```yaml
Responsable: Backend Dev + DevOps
Dur√©e: 2 jours
Pr√©requis: Core engine complet

Actions:
  - Activer mode DRY_RUN=true par d√©faut
  - Logging d√©taill√© de toutes les recommandations
  - Comparaison recommandations vs actions manuelles
  - Dashboard visualisation d√©cisions

Configuration:
  DRY_RUN=true  # Aucune action r√©elle
  LOG_LEVEL=DEBUG
  SHADOW_MODE_ENABLED=true
  SHADOW_MODE_LOG_ALL_DECISIONS=true

Logs √† capturer:
  - Recommandations g√©n√©r√©es
  - Score de chaque canal
  - D√©cision sugg√©r√©e (avec raison)
  - √âtat actuel vs √©tat recommand√©
  - Timestamp et contexte

Crit√®res de succ√®s:
  - ‚úÖ Aucune action r√©elle ex√©cut√©e
  - ‚úÖ 100% des recommandations logg√©es
  - ‚úÖ Dashboard temps r√©el fonctionnel
  - ‚úÖ Export quotidien des rapports

Fichiers:
  - config.py (ajouter SHADOW_MODE flag)
  - src/tools/shadow_mode_logger.py (nouveau)
  - app/routes/shadow_dashboard.py (nouveau)
```

**3.1.2 Collecte et Analyse M√©triques**

```yaml
Responsable: Data Analyst + Backend Dev
Dur√©e: Continu (14-21 jours)
Pr√©requis: Shadow mode actif

Actions:
  - Collection quotidienne m√©triques
  - Analyse recommandations vs r√©alit√©
  - Identification patterns et anomalies
  - Ajustement heuristiques si n√©cessaire

M√©triques √† tracker:
  - Nombre de recommandations par type
  - Distribution des scores
  - Canaux concern√©s (IDs et stats)
  - Timing des recommandations
  - Corr√©lation avec events r√©seau

Analyses quotidiennes:
  - Recommandations align√©es avec intuition?
  - Faux positifs / faux n√©gatifs
  - Cas limites identifi√©s
  - Performance des heuristiques

Crit√®res de succ√®s:
  - ‚úÖ Rapport quotidien g√©n√©r√© automatiquement
  - ‚úÖ Taux de faux positifs < 10%
  - ‚úÖ Recommandations "sens√©es" > 90%
  - ‚úÖ Pas de recommandations aberrantes

Fichiers:
  - scripts/daily_shadow_report.py (nouveau)
  - data/reports/shadow_mode/ (nouveau dossier)
  - docs/shadow-mode-analysis.md (mis √† jour quotidien)
```

**3.1.3 Validation avec Experts**

```yaml
Responsable: Product Owner + Node Operators
Dur√©e: 1 semaine (fin de shadow mode)
Pr√©requis: 14+ jours de donn√©es shadow

Actions:
  - Review √©chantillon de recommandations
  - Comparaison avec d√©cisions manuelles
  - Identification cas √† am√©liorer
  - Validation globale syst√®me

Review process:
  - S√©lection al√©atoire 100 recommandations
  - √âvaluation par expert: Agree/Disagree/Unsure
  - Discussion cas de d√©saccord
  - Ajustement thresholds si n√©cessaire

Crit√®res d'acceptation:
  - ‚úÖ Agreement rate > 80%
  - ‚úÖ Aucun cas critique manqu√©
  - ‚úÖ Pas de recommandations dangereuses
  - ‚úÖ Green light pour phase suivante

Fichiers:
  - docs/shadow-mode-validation-report.md (nouveau)
```

### 3.2 Tests avec N≈ìud R√©el

#### Objectif
Valider le syst√®me avec un vrai n≈ìud Lightning en conditions r√©elles.

#### T√¢ches

**3.2.1 Setup N≈ìud de Test**

```yaml
Responsable: DevOps + Node Operator
Dur√©e: 2 jours
Pr√©requis: Shadow mode valid√©

Actions:
  - S√©lection n≈ìud de test (non-critique)
  - Configuration connexion LND/LNBits
  - Isolation du n≈ìud (pas de impact prod)
  - Backup complet de l'√©tat initial

Crit√®res de s√©lection n≈ìud:
  - Non-critique (pas de routing majeur)
  - Capacit√© modeste (< 5M sats)
  - Quelques canaux actifs (5-10)
  - Possibilit√© de rollback complet

Configuration:
  NODE_ID=03abc...
  NODE_TYPE=lnd
  LND_REST_URL=https://node-test.example.com:8080
  LNBITS_URL=https://lnbits-test.example.com
  TEST_MODE=true  # Restrictions suppl√©mentaires

Crit√®res de succ√®s:
  - ‚úÖ Connexion √©tablie et v√©rifi√©e
  - ‚úÖ Backup complet effectu√©
  - ‚úÖ Restrictions de s√©curit√© actives
  - ‚úÖ Monitoring d√©di√© configur√©

Fichiers:
  - config/node_test.yaml (nouveau)
  - scripts/backup_node_state.sh (nouveau)
```

**3.2.2 Test Pilote (1 Canal)**

```yaml
Responsable: Backend Dev + Node Operator
Dur√©e: 1 semaine
Pr√©requis: N≈ìud de test configur√©

Actions:
  - Activation sur 1 canal uniquement
  - Mode semi-automatique (confirmation manuelle)
  - Observation impact pendant 7 jours
  - Rollback si probl√®me

Workflow:
  1. S√©lection canal test (crit√®res: faible volume)
  2. MCP g√©n√®re recommandation
  3. Validation manuelle requise
  4. Application si approuv√©e
  5. Observation 48h minimum
  6. Mesure impact (forwards, fees earned)

M√©triques √† comparer (avant/apr√®s):
  - Forward success rate
  - Forward volume (sats/jour)
  - Fees earned (sats/jour)
  - Balance stability
  - Peer satisfaction (indirect)

Crit√®res de succ√®s:
  - ‚úÖ Pas de d√©gradation performance
  - ‚úÖ Am√©lioration >= 10% sur au moins 2 m√©triques
  - ‚úÖ Aucun incident ou crash
  - ‚úÖ Rollback fonctionnel si test√©

Fichiers:
  - scripts/pilot_test_single_channel.py (nouveau)
  - data/reports/pilot_test/ (r√©sultats)
```

**3.2.3 Expansion Progressive**

```yaml
Responsable: Node Operator
Dur√©e: 2-3 semaines
Pr√©requis: Test pilote 1 canal r√©ussi

Actions:
  - Expansion √† 3 canaux (semaine 1)
  - Expansion √† 5 canaux (semaine 2)
  - Expansion √† tous canaux n≈ìud test (semaine 3)
  - √âvaluation globale

Progression:
  Week 1: 1 canal ‚Üí 3 canaux (diversifi√©s)
  Week 2: 3 canaux ‚Üí 5 canaux
  Week 3: 5 canaux ‚Üí tous canaux (si ok)

Crit√®res d'expansion:
  - Aucun incident sur phase pr√©c√©dente
  - Am√©lioration nette confirm√©e
  - Validation manuelle de chaque √©tape
  - Monitoring sans alertes critiques

Crit√®res de succ√®s (fin semaine 3):
  - ‚úÖ Tous canaux sous MCP (ou rollback si √©chec)
  - ‚úÖ Am√©lioration globale n≈ìud > 15%
  - ‚úÖ Aucun incident critique
  - ‚úÖ Satisfaction node operator

Fichiers:
  - docs/pilot-expansion-report.md (hebdomadaire)
```

### 3.3 Activation Production Limit√©e

#### Objectif
Activer MCP sur un nombre limit√© de n≈ìuds production avec garde-fous stricts.

#### T√¢ches

**3.3.1 Crit√®res de Qualification N≈ìuds**

```yaml
Responsable: Product Owner
Dur√©e: 1 jour
Pr√©requis: Tests pilotes concluants

Actions:
  - D√©finition crit√®res qualification
  - S√©lection premiers n≈ìuds candidats
  - Validation par node operators
  - Onboarding et formation

Crit√®res de qualification:
  - N≈ìud mature (> 6 mois)
  - Capacit√© mod√©r√©e (1-10M sats)
  - Nombre canaux g√©rable (10-50)
  - Node operator exp√©riment√©
  - Acceptation termes (beta, monitoring)

Liste d'attente:
  - Max 5 n≈ìuds pour v1.0 initial
  - Diversit√© g√©ographique et taille
  - Mix entre hubs et n≈ìuds routeurs

Crit√®res de succ√®s:
  - ‚úÖ Crit√®res document√©s et valid√©s
  - ‚úÖ 5 n≈ìuds qualifi√©s et acceptant
  - ‚úÖ Contrat/termes beta sign√©s

Fichiers:
  - docs/node-qualification-criteria.md (nouveau)
  - data/qualified_nodes.yaml (nouveau)
```

**3.3.2 Mode Semi-Automatique**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: N≈ìuds qualifi√©s identifi√©s

Actions:
  - Impl√©mentation workflow approbation
  - Interface confirmation node operator
  - Notifications avant chaque action
  - Timeout si pas de r√©ponse (24h ‚Üí skip)

Workflow approbation:
  1. MCP g√©n√®re recommandation
  2. Notification Telegram √† node operator
  3. Dashboard affiche d√©tails + contexte
  4. Operator a 24h pour approuver/rejeter
  5. Si approuv√©: ex√©cution automatique
  6. Si rejet√©: skip + feedback collect√©
  7. Si timeout: skip (aucune action)

Interface:
  - Telegram bot avec boutons Approve/Reject
  - Dashboard web avec d√©tails complets
  - Historique d√©cisions pass√©es
  - Feedback form si rejection

Crit√®res de succ√®s:
  - ‚úÖ Workflow approbation fonctionnel
  - ‚úÖ Notifications fiables (100%)
  - ‚úÖ Interface intuitive (UX valid√©e)
  - ‚úÖ Aucune action sans confirmation

Fichiers:
  - src/approval/approval_workflow.py (nouveau)
  - app/routes/approval_dashboard.py (nouveau)
  - src/integrations/telegram_bot.py (am√©liorer)
```

**3.3.3 Monitoring et Alertes Avanc√©es**

```yaml
Responsable: DevOps + Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Mode semi-auto impl√©ment√©

Actions:
  - Alertes multi-niveaux (info, warning, critical)
  - Dashboard temps r√©el par n≈ìud
  - D√©tection anomalies automatique
  - Escalation si probl√®me

Niveaux d'alertes:
  INFO: Action appliqu√©e avec succ√®s
  WARNING: M√©trique inhabituelle (pas critique)
  CRITICAL: D√©gradation performance, √©chec action
  
Alertes automatiques si:
  - Forward success rate drop > 20%
  - Fees earned drop > 30%
  - Error rate > 5%
  - Latency spike > 2x normale
  - Service unavailable

Canaux de notification:
  - Telegram (tous niveaux)
  - Email (warning + critical)
  - Slack (critical uniquement)
  - PagerDuty (critical + no response 15min)

Crit√®res de succ√®s:
  - ‚úÖ D√©tection anomalie < 5 min
  - ‚úÖ Notification < 2 min apr√®s d√©tection
  - ‚úÖ Dashboard temps r√©el fonctionnel
  - ‚úÖ Escalation process test√©

Fichiers:
  - src/monitoring/anomaly_detector.py (nouveau)
  - src/monitoring/alert_manager.py (am√©liorer)
  - app/routes/realtime_dashboard.py (nouveau)
```

---

## PRIORIT√â 4 - FONCTIONNALIT√âS AVANC√âES

> **Dur√©e estim√©e** : 4-6 semaines  
> **Criticit√©** : üü¢ MOYENNE  
> **D√©pendances** : Priorit√© 3 r√©ussie

### 4.1 Syst√®me RAG Lightning Complet

#### Objectif
Activer le syst√®me RAG (Retrieval-Augmented Generation) pour analyses contextuelles avanc√©es.

#### T√¢ches

**4.1.1 Activation RAG Backend**

```yaml
Responsable: ML Engineer + Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Qdrant configur√©, donn√©es disponibles

Actions:
  - Activer rag/ system complet
  - Configuration Qdrant vector store
  - Ingestion documents Lightning existants
  - Tests end-to-end RAG pipeline

Documents √† ing√©rer:
  - Documentation Lightning Network (BOLT specs)
  - Analyses historiques n≈ìuds
  - Best practices fee optimization
  - Rapports shadow mode
  - Donn√©es r√©seau agr√©g√©es

Pipeline RAG:
  1. Document chunking (512 tokens)
  2. Embedding g√©n√©ration (Anthropic/OpenAI)
  3. Indexation Qdrant
  4. Query expansion
  5. Retrieval (top-k=5)
  6. Reranking
  7. Context injection
  8. LLM generation

Crit√®res de succ√®s:
  - ‚úÖ Qdrant healthy et performant
  - ‚úÖ Documents index√©s (> 1000)
  - ‚úÖ Retrieval latency < 500ms
  - ‚úÖ Relevance score > 0.8

Fichiers:
  - rag/ (activer tous modules)
  - rag/ingest_lightning_docs.py (nouveau)
  - config/rag_config.yaml (nouveau)
```

**4.1.2 Endpoints RAG API**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: RAG backend actif

Actions:
  - Cr√©er endpoints /api/v1/rag/
  - Query Lightning Network knowledge
  - Analyses contextuelles n≈ìuds/canaux
  - Recommandations enrichies par RAG

Endpoints:
  POST /api/v1/rag/query
    Body: {"query": "How to optimize fees for high-volume channel?"}
    Response: {"answer": "...", "sources": [...], "confidence": 0.95}
  
  POST /api/v1/rag/analyze/node/{node_id}
    Response: Analyse contextuelle compl√®te du n≈ìud
  
  POST /api/v1/rag/analyze/channel/{channel_id}
    Response: Recommandations enrichies RAG

Crit√®res de succ√®s:
  - ‚úÖ Tous endpoints fonctionnels
  - ‚úÖ Response time < 2s (p95)
  - ‚úÖ Answers pertinentes (validation manuelle)
  - ‚úÖ Sources trac√©es et v√©rifiables

Fichiers:
  - app/routes/rag.py (compl√©ter)
  - app/services/rag_service.py (am√©liorer)
  - tests/test_rag_endpoints.py (nouveau)
```

**4.1.3 Int√©gration RAG dans Optimizer**

```yaml
Responsable: Backend Dev + ML Engineer
Dur√©e: 1 semaine
Pr√©requis: Endpoints RAG fonctionnels

Actions:
  - Enrichir recommandations avec contexte RAG
  - Explications en langage naturel
  - Confidence scores am√©lior√©s
  - Comparaison avec best practices

Workflow enrichi:
  1. Optimizer g√©n√®re recommandation (score)
  2. Query RAG avec contexte canal
  3. RAG enrichit avec best practices
  4. G√©n√©ration explication claire
  5. Confidence score ajust√©
  6. Retour recommandation + explication

Exemple output:
  {
    "channel_id": "...",
    "current_fee": 1000,
    "recommended_fee": 500,
    "confidence": 0.87,
    "reasoning": "Based on network data, channels with similar characteristics (high volume, reliable peer) perform better with lower fees. Your current fee is 2x the network median for comparable channels.",
    "sources": ["Network stats 2025-10", "Best practices doc"],
    "estimated_impact": "+25% forward volume"
  }

Crit√®res de succ√®s:
  - ‚úÖ Explications claires et actionnables
  - ‚úÖ Confidence scores calibr√©s
  - ‚úÖ Sources v√©rifiables
  - ‚úÖ Feedback positif node operators

Fichiers:
  - src/optimizers/rag_enriched_optimizer.py (nouveau)
```

### 4.2 Int√©grations Externes Avanc√©es

#### Objectif
Connecter les APIs externes pour enrichissement de donn√©es temps r√©el.

#### T√¢ches

**4.2.1 Int√©gration Amboss API Compl√®te**

```yaml
Responsable: Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Compte Amboss API

Actions:
  - Client Amboss complet (tous endpoints)
  - Sync donn√©es n≈ìuds temps r√©el
  - R√©cup√©ration m√©triques avanc√©es
  - Cache intelligent

Endpoints Amboss utilis√©s:
  - GET /nodes/{pubkey}
  - GET /nodes/{pubkey}/health
  - GET /nodes/{pubkey}/channels
  - GET /network/stats
  - GET /rankings

Donn√©es r√©cup√©r√©es:
  - Node capacity, channel count
  - Centrality metrics
  - Health score Amboss
  - Rankings position
  - Historical data

Cache strategy:
  - Node data: 1h TTL
  - Health scores: 15min TTL
  - Network stats: 6h TTL
  - Rankings: 24h TTL

Crit√®res de succ√®s:
  - ‚úÖ Tous endpoints impl√©ment√©s
  - ‚úÖ Rate limit respect√© (configur√© par tier)
  - ‚úÖ Cache hit rate > 85%
  - ‚úÖ Fallback si API down

Variables .env:
  AMBOSS_API_KEY=your_api_key
  AMBOSS_API_URL=https://api.amboss.space/graphql
  AMBOSS_RATE_LIMIT=100  # req/min

Fichiers:
  - src/clients/amboss_client.py (compl√©ter)
  - tests/test_amboss_client.py (nouveau)
```

**4.2.2 Int√©gration Mempool.space**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: Aucune d√©pendance externe

Actions:
  - Client Mempool.space API
  - Donn√©es on-chain pour n≈ìuds
  - Channel open/close d√©tection
  - Fee estimations on-chain

Endpoints utilis√©s:
  - GET /api/v1/lightning/nodes/{pubkey}
  - GET /api/v1/lightning/channels/{id}
  - GET /api/v1/fees/recommended

Use cases:
  - D√©tection channels r√©cents (< 7j)
  - √Çge r√©el des channels (block height)
  - On-chain fees pour close recommendations
  - Network stats globales

Crit√®res de succ√®s:
  - ‚úÖ Client fonctionnel
  - ‚úÖ Donn√©es on-chain correctes
  - ‚úÖ Integration avec optimizer
  - ‚úÖ Cache appropri√© (6h TTL)

Fichiers:
  - src/clients/mempool_client.py (nouveau)
  - src/integrations/onchain_data.py (nouveau)
```

**4.2.3 Int√©gration 1ML (optionnelle)**

```yaml
Responsable: Backend Dev
Dur√©e: 2 jours
Pr√©requis: Aucune

Actions:
  - Client 1ML API
  - Rankings et stats alternatives
  - Comparaison multi-sources
  - Enrichissement profils n≈ìuds

Endpoints:
  - GET /node/{pubkey}
  - GET /statistics

Crit√®res de succ√®s:
  - ‚úÖ Client fonctionnel
  - ‚úÖ Donn√©es agr√©g√©es avec Amboss
  - ‚úÖ Comparaison rankings multi-sources

Fichiers:
  - src/clients/oneml_client.py (nouveau)
```

### 4.3 Monitoring et Observabilit√© Complets

#### Objectif
Impl√©menter un syst√®me de monitoring production-grade avec m√©triques d√©taill√©es.

#### T√¢ches

**4.3.1 Prometheus Metrics Complet**

```yaml
Responsable: DevOps + Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Infrastructure stable

Actions:
  - Instrumenter toute l'application
  - Export m√©triques Prometheus format
  - Endpoint /metrics optimis√©
  - Labels et cardinality appropri√©s

M√©triques √† exposer:

  # HTTP Metrics
  http_requests_total{method, endpoint, status}
  http_request_duration_seconds{method, endpoint}
  http_request_size_bytes{method, endpoint}
  http_response_size_bytes{method, endpoint}
  
  # Application Metrics
  mcp_optimizations_total{node_id, decision_type}
  mcp_optimization_duration_seconds{node_id}
  mcp_channels_analyzed_total
  mcp_decisions_applied_total{result}
  mcp_rollbacks_total{reason}
  
  # Lightning Metrics
  lightning_node_score{node_id}
  lightning_channel_score{channel_id}
  lightning_forward_success_rate{node_id}
  lightning_fees_earned_sats{node_id}
  
  # External API Metrics
  external_api_requests_total{provider, endpoint, status}
  external_api_duration_seconds{provider, endpoint}
  external_api_errors_total{provider, error_type}
  
  # Cache Metrics
  cache_hits_total{cache_type}
  cache_misses_total{cache_type}
  cache_hit_rate{cache_type}
  
  # Database Metrics
  db_queries_total{operation, collection}
  db_query_duration_seconds{operation, collection}
  db_connection_pool_size
  db_connection_pool_active

Crit√®res de succ√®s:
  - ‚úÖ > 50 m√©triques expos√©es
  - ‚úÖ Endpoint /metrics < 100ms
  - ‚úÖ Cardinality contr√¥l√©e (< 1000 labels)
  - ‚úÖ Documentation compl√®te m√©triques

Fichiers:
  - src/monitoring/prometheus_exporter.py (nouveau)
  - app/middleware/metrics_middleware.py (nouveau)
  - docs/metrics-documentation.md (nouveau)
```

**4.3.2 Grafana Dashboard**

```yaml
Responsable: DevOps
Dur√©e: 1 semaine
Pr√©requis: Prometheus configur√©

Actions:
  - Cr√©er dashboards Grafana
  - Panels pour toutes m√©triques cl√©s
  - Alertes configur√©es
  - Templates exportables

Dashboards √† cr√©er:

  1. MCP Overview
     - Request rate, error rate, latency
     - Active nodes, channels analyzed
     - Decisions applied, rollbacks
     - Cache hit rate, DB performance
  
  2. Lightning Performance
     - Node scores distribution
     - Channel scores distribution
     - Forward success rates
     - Fees earned evolution
  
  3. External APIs
     - Request rates par provider
     - Latencies par endpoint
     - Error rates
     - Rate limit consumption
  
  4. System Health
     - CPU, Memory, Disk usage
     - Connection pools
     - Queue sizes
     - Circuit breaker states
  
  5. Business Metrics
     - Optimizations per day/week
     - Average improvement per node
     - User satisfaction scores
     - Revenue impact

Alertes Grafana:
  - Error rate > 5% (5min)
  - Latency p95 > 2s (5min)
  - Service down (1min)
  - Database slow queries > 10s
  - External API failures > 20% (10min)

Crit√®res de succ√®s:
  - ‚úÖ 5 dashboards complets
  - ‚úÖ Alertes fonctionnelles
  - ‚úÖ Templates JSON export√©s
  - ‚úÖ Documentation usage

Fichiers:
  - monitoring/grafana/dashboards/*.json (nouveau)
  - monitoring/grafana/alerts/*.yaml (nouveau)
  - docs/grafana-setup.md (nouveau)
```

**4.3.3 Log Aggregation et Analysis**

```yaml
Responsable: DevOps + Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Structured logging en place

Actions:
  - Centraliser logs (ELK ou Loki)
  - Structured logging (JSON)
  - Log levels appropri√©s
  - Retention policy

Stack logging:
  - Loki (log aggregation)
  - Promtail (log shipper)
  - Grafana (visualization)
  - Ou alternative: ELK Stack

Format logs:
  {
    "timestamp": "2025-10-12T10:30:00Z",
    "level": "INFO",
    "logger": "mcp.optimizer",
    "message": "Optimization completed",
    "node_id": "03abc...",
    "duration_ms": 234,
    "decisions": 3,
    "trace_id": "xyz-123"
  }

Log levels:
  DEBUG: D√©tails techniques (dev only)
  INFO: Events normaux (optimization start/end)
  WARNING: Situations inhabituelles (high latency)
  ERROR: Erreurs r√©cup√©rables (API timeout)
  CRITICAL: Erreurs critiques (DB down)

Retention:
  - INFO: 7 jours
  - WARNING: 30 jours
  - ERROR/CRITICAL: 90 jours
  - DEBUG: 1 jour (production off)

Crit√®res de succ√®s:
  - ‚úÖ Tous logs centralis√©s
  - ‚úÖ Recherche rapide (< 1s)
  - ‚úÖ Alertes sur patterns (error spikes)
  - ‚úÖ Retention respect√©e

Fichiers:
  - monitoring/loki-config.yaml (nouveau)
  - monitoring/promtail-config.yaml (nouveau)
  - src/logging_config.py (am√©liorer)
```

### 4.4 Performance et Scaling

#### Objectif
Optimiser les performances et pr√©parer le syst√®me pour le scaling.

#### T√¢ches

**4.4.1 Optimisations Cache Avanc√©es**

```yaml
Responsable: Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Redis configur√©

Actions:
  - Impl√©mentation cache multi-niveaux
  - Cache warming automatique
  - TTL dynamiques bas√©s sur volatilit√©
  - Cache invalidation intelligente

Architecture cache:

  Level 1 (Memory): In-process LRU
    - Donn√©es ultra-fr√©quentes
    - TTL: 1-5 min
    - Size: 100MB max
    - Use: Config, thresholds
  
  Level 2 (Redis): Distributed cache
    - Donn√©es fr√©quentes
    - TTL: 5min - 6h
    - Size: Illimit√© (managed)
    - Use: Node data, scores, graphs
  
  Level 3 (DB): Source of truth
    - Donn√©es permanentes
    - No TTL
    - Use: Policies, decisions, metrics

Cache warming:
  - Pr√©calcul des top 100 nodes (6h)
  - Network graph (6h)
  - Popular queries (analytics)

TTL dynamiques:
  - Donn√©es volatiles (prices): 1min
  - Donn√©es stables (node capacity): 1h
  - Donn√©es immuables (old decisions): 24h

Invalidation:
  - Event-based (policy applied ‚Üí invalidate node cache)
  - Time-based (TTL expiration)
  - Manual (API endpoint)

Crit√®res de succ√®s:
  - ‚úÖ Hit rate global > 85%
  - ‚úÖ Latency reduction 60%
  - ‚úÖ Cache memory usage < 500MB
  - ‚úÖ Automatic warming fonctionnel

Fichiers:
  - src/cache/multi_level_cache.py (nouveau)
  - src/cache/cache_warming.py (nouveau)
  - src/cache/invalidation_manager.py (nouveau)
```

**4.4.2 Database Connection Pooling**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: MongoDB/Redis actifs

Actions:
  - Configurer connection pools optimaux
  - Monitoring pool utilization
  - Auto-scaling pool size
  - Retry logic et circuit breakers

Configuration MongoDB:
  min_pool_size: 10
  max_pool_size: 100
  max_idle_time_ms: 60000
  wait_queue_timeout_ms: 5000
  
Configuration Redis:
  max_connections: 50
  timeout: 5
  retry_on_timeout: true
  health_check_interval: 30

Crit√®res de succ√®s:
  - ‚úÖ Pool utilization 50-70% moyenne
  - ‚úÖ No connection timeouts
  - ‚úÖ Graceful degradation si saturation
  - ‚úÖ Metrics pool expos√©es Prometheus

Fichiers:
  - database.py (am√©liorer)
  - src/db/connection_manager.py (nouveau)
```

**4.4.3 Background Tasks Asynchrones**

```yaml
Responsable: Backend Dev
Dur√©e: 1 semaine
Pr√©requis: Infrastructure stable

Actions:
  - Impl√©mentation task queue (Celery/RQ)
  - T√¢ches lourdes en background
  - Scheduling optimisations p√©riodiques
  - Monitoring tasks

T√¢ches en background:
  - Network graph sync (6h)
  - Score recalculation (1h)
  - Daily reports g√©n√©ration
  - Cleanup old data (24h)
  - Cache warming (6h)

Stack:
  - Celery (task queue)
  - Redis (broker)
  - Flower (monitoring)

Configuration:
  workers: 4
  concurrency: 8 (gevent)
  task_time_limit: 600
  task_soft_time_limit: 300

Crit√®res de succ√®s:
  - ‚úÖ T√¢ches lourdes n'impactent pas API
  - ‚úÖ Scheduling fiable
  - ‚úÖ Retry automatique sur √©chec
  - ‚úÖ Monitoring tasks temps r√©el

Fichiers:
  - src/tasks/ (nouveau dossier)
    - celery_app.py
    - optimization_tasks.py
    - sync_tasks.py
    - report_tasks.py
  - requirements.txt (ajouter celery, flower)
```

**4.4.4 Rate Limiting et Throttling**

```yaml
Responsable: Backend Dev
Dur√©e: 3 jours
Pr√©requis: Redis actif

Actions:
  - Rate limiting par endpoint
  - Throttling requ√™tes lourdes
  - Protection contre abuse
  - Documentation limits

Strat√©gie:
  - Public endpoints: 100 req/min
  - Authenticated: 1000 req/min
  - Heavy endpoints (optimization): 10 req/min
  - Background tasks: No limit

Impl√©mentation:
  - Token bucket algorithm
  - Redis pour state distribu√©
  - Headers informatifs (X-RateLimit-*)
  - 429 response si exceeded

Headers:
  X-RateLimit-Limit: 1000
  X-RateLimit-Remaining: 987
  X-RateLimit-Reset: 1699876543

Crit√®res de succ√®s:
  - ‚úÖ Rate limiting fonctionnel
  - ‚úÖ Pas d'impact performance l√©gitimes users
  - ‚úÖ Documentation limites API
  - ‚úÖ Metrics rate limit violations

Fichiers:
  - app/middleware/rate_limiter.py (nouveau)
  - docs/api-rate-limits.md (nouveau)
```

---

## TIMELINE & RESSOURCES

### Planning Global

| Phase | D√©but | Dur√©e | Fin |
|-------|-------|-------|-----|
| **Priorit√© 1** | S+0 | 2 semaines | S+2 |
| **Priorit√© 2** | S+2 | 3 semaines | S+5 |
| **Priorit√© 3** | S+5 | 4 semaines | S+9 |
| **Priorit√© 4** | S+9 | 6 semaines | S+15 |

### Allocation Ressources

| Phase | Dur√©e | Backend Dev | DevOps | ML Engineer | QA | Total FTE |
|-------|-------|-------------|--------|-------------|----|----|
| **P1** | 2 semaines | 1.0 | 1.0 | 0 | 0.5 | 2.5 |
| **P2** | 3 semaines | 2.0 | 0.5 | 0 | 0.5 | 3.0 |
| **P3** | 4 semaines | 1.0 | 0.5 | 0 | 1.0 | 2.5 |
| **P4** | 6 semaines | 1.5 | 0.5 | 1.0 | 0.5 | 3.5 |
| **Total** | 15 semaines | - | - | - | - | ~3 FTE avg |

### Budget Estim√©

```yaml
Services Cloud (mensuel):
  MongoDB Atlas M10: $60/mois
  Redis Cloud 250MB: $10/mois
  Compute (VPS): $40/mois
  Qdrant Cloud: $50/mois (optionnel)
  Total: ~$160/mois

APIs Externes (mensuel):
  Amboss API: $50-200/mois (selon tier)
  Anthropic API: $50-500/mois (usage)
  Total: ~$100-700/mois

Infrastructure:
  Monitoring (Grafana Cloud): $50/mois ou self-hosted
  Backups: $20/mois
  SSL Certs: $0 (Let's Encrypt)
  Total: ~$20-70/mois

TOTAL MENSUEL: $280-930/mois
```

### Milestones Cl√©s

| Date Cible | Milestone | Crit√®re de Succ√®s |
|------------|-----------|-------------------|
| **S+2** | Infrastructure Stable | API HTTPS accessible, systemd configur√©, Docker rebuild r√©ussi |
| **S+5** | Core Engine Complet | LNBits int√©gr√©, heuristiques finalis√©es, tests valid√©s |
| **S+9** | Shadow Mode Valid√© | 21 jours observation, validation experts, > 80% agreement |
| **S+12** | Production Limit√©e Active | 5 n≈ìuds en production, mode semi-auto, monitoring complet |
| **S+15** | v1.0 Feature Complete | RAG actif, int√©grations externes, monitoring Grafana |

---

## CRIT√àRES DE SUCC√àS

### Succ√®s par Phase

#### Phase 1 - Infrastructure ‚úÖ

```yaml
Crit√®res obligatoires:
  - ‚úÖ API accessible via HTTPS (uptime > 99%)
  - ‚úÖ Service systemd auto-restart fonctionnel
  - ‚úÖ Image Docker stable (0 crashes)
  - ‚úÖ MongoDB & Redis connect√©s (latency < 50ms)
  - ‚úÖ Mode d√©grad√© fonctionnel (fallback)

Crit√®res optionnels:
  - ‚≠ê Monitoring infrastructure (Grafana)
  - ‚≠ê Automated backups configur√©s
  - ‚≠ê Multi-region deployment (HA)
```

#### Phase 2 - Core Engine ‚úÖ

```yaml
Crit√®res obligatoires:
  - ‚úÖ LNBits client complet (100% endpoints)
  - ‚úÖ Authentification macaroon s√©curis√©e
  - ‚úÖ Heuristiques impl√©ment√©es (8 heuristiques min)
  - ‚úÖ Decision engine valid√© (tests > 95%)
  - ‚úÖ Rollback fonctionnel (< 30s)
  - ‚úÖ Lightning scoring actif

Crit√®res optionnels:
  - ‚≠ê Calibration heuristiques sur > 1000 canaux
  - ‚≠ê Scoring r√©seau complet (centrality, etc.)
```

#### Phase 3 - Production Contr√¥l√©e ‚úÖ

```yaml
Crit√®res obligatoires:
  - ‚úÖ Shadow mode 21 jours minimum
  - ‚úÖ Validation experts (> 80% agreement)
  - ‚úÖ Test pilote 1 canal r√©ussi
  - ‚úÖ Expansion progressive valid√©e
  - ‚úÖ 5 n≈ìuds en production
  - ‚úÖ Mode semi-auto fonctionnel
  - ‚úÖ Alertes actives et test√©es

Crit√®res optionnels:
  - ‚≠ê 10+ n≈ìuds en production
  - ‚≠ê Mode fully-auto (pour n≈ìuds consentants)
```

#### Phase 4 - Fonctionnalit√©s Avanc√©es ‚úÖ

```yaml
Crit√®res obligatoires:
  - ‚úÖ RAG syst√®me actif (queries fonctionnelles)
  - ‚úÖ Int√©gration Amboss compl√®te
  - ‚úÖ Monitoring Prometheus + Grafana
  - ‚úÖ Cache multi-niveaux (hit rate > 85%)

Crit√®res optionnels:
  - ‚≠ê Int√©grations multiples (1ML, Sparkseer)
  - ‚≠ê Background tasks (Celery)
  - ‚≠ê Rate limiting avanc√©
```

### M√©triques de Succ√®s Globales

```yaml
Performance:
  - API uptime: > 99.5%
  - Response time p95: < 500ms
  - Response time p99: < 2s
  - Error rate: < 0.5%
  - Cache hit rate: > 85%

Fonctionnalit√©:
  - Optimizations/jour: > 50
  - Nodes actifs: > 5
  - Canaux optimis√©s: > 100
  - Success rate optimizations: > 95%

Business:
  - Am√©lioration moyenne fees: > +15%
  - Am√©lioration forward rate: > +20%
  - Satisfaction node operators: > 80%
  - Faux positifs: < 5%

Qualit√©:
  - Code coverage: > 85%
  - Tests passing: 100%
  - S√©curit√©: 0 vuln√©rabilit√©s critiques
  - Documentation: 100% endpoints document√©s
```

---

## ANNEXES

### A. Risques et Mitigations

| Risque | Probabilit√© | Impact | Mitigation |
|--------|-------------|--------|------------|
| Image Docker build √©choue | Moyenne | Haut | Fallback Python direct, rebuild incr√©mental |
| MongoDB/Redis indispo | Faible | Moyen | Mode d√©grad√©, fallback local |
| LNBits API rate limit | Moyenne | Moyen | Cache agressif, retry logic |
| Faux positifs recommandations | Moyenne | Haut | Shadow mode extended, validation experts |
| Performance d√©grad√©e production | Faible | Haut | Load testing pr√©alable, monitoring alertes |
| Node operators insatisfaits | Moyenne | Haut | Mode semi-auto, feedback continu |

### B. D√©pendances Externes

```yaml
Services Cloud:
  - MongoDB Atlas: SLA 99.95%
  - Redis Cloud/Upstash: SLA 99.9%
  - Qdrant Cloud: SLA 99.5% (optionnel)

APIs Externes:
  - LNBits: Disponibilit√© requise 99%+
  - Amboss API: Rate limits variables (selon tier)
  - Mempool.space: Public API, best effort
  - Anthropic API: SLA 99.9%

Infrastructure:
  - Serveur production: Hostinger VPS
  - Domain & SSL: Let's Encrypt
  - Monitoring: Self-hosted ou Grafana Cloud
```

### C. Checklist Go-Live

```markdown
## Checklist Pre-Production

### Infrastructure
- [ ] HTTPS configur√© et test√© (SSL A+)
- [ ] Systemd service activ√© et test√©
- [ ] Docker image rebuilt et d√©ploy√©e
- [ ] MongoDB Atlas production ready
- [ ] Redis Cloud production ready
- [ ] Backups automatiques configur√©s
- [ ] Monitoring alertes actives

### Code
- [ ] Tous tests passent (100%)
- [ ] Code coverage > 85%
- [ ] Code review compl√©t√©
- [ ] Documentation √† jour
- [ ] Security audit pass√©
- [ ] Performance tests valid√©s

### Int√©grations
- [ ] LNBits connexion valid√©e
- [ ] Authentification macaroon test√©e
- [ ] Amboss API fonctionnelle
- [ ] Mempool.space int√©gr√©
- [ ] RAG queries test√©es

### Monitoring
- [ ] Prometheus metrics expos√©es
- [ ] Grafana dashboards cr√©√©s
- [ ] Alertes configur√©es et test√©es
- [ ] Logs aggregation active
- [ ] Retention policies configur√©es

### S√©curit√©
- [ ] Credentials chiffr√©es
- [ ] Rate limiting actif
- [ ] CORS configur√©
- [ ] Headers s√©curit√© (HSTS, CSP)
- [ ] Audit logs activ√©s

### Validation
- [ ] Shadow mode 21 jours compl√©t√©
- [ ] Validation experts (> 80%)
- [ ] Test pilote r√©ussi
- [ ] Rollback test√©
- [ ] Disaster recovery document√©

### Documentation
- [ ] API documentation (Swagger)
- [ ] Runbooks op√©rationnels
- [ ] Troubleshooting guide
- [ ] Metrics documentation
- [ ] User guides (node operators)
```

### D. Contacts et Escalation

```yaml
√âquipe:
  Backend Lead: [Contact]
  DevOps Lead: [Contact]
  ML Engineer: [Contact]
  Product Owner: [Contact]

Support:
  Niveau 1: Monitoring automatique + Telegram
  Niveau 2: On-call engineer (24/7)
  Niveau 3: Backend Lead + DevOps Lead

Escalation:
  - Incident mineur: Slack notification
  - Incident majeur: Email + Slack + Telegram
  - Incident critique: PagerDuty + Phone call

SLA R√©ponse:
  - P0 (Critical, production down): 15 min
  - P1 (Major, degraded service): 1h
  - P2 (Minor, workaround exists): 4h
  - P3 (Low, cosmetic): 24h
```

### E. R√©f√©rences

```markdown
Documents Techniques:
  - _SPECS/Plan-MVP.md
  - docs/backbone-technique-MVP.md
  - docs/dictionnaire-donnees.md
  - production_optimization_audit.md

Code:
  - src/optimizers/core_fee_optimizer.py
  - src/clients/lnbits_client.py
  - app/services/lightning_scoring.py
  - rag/ (syst√®me RAG)

Scripts:
  - scripts/configure_nginx_production.sh
  - scripts/configure_systemd_autostart.sh
  - monitor_production.py

Rapports:
  - RAPPORT_FINAL_RESOLUTION_10OCT2025.md
  - INVESTIGATION_FINALE_10OCT2025.md
  - PHASE5-STATUS.md
```

---

**Document Version**: 1.0.0  
**Date**: 12 octobre 2025  
**Auteur**: √âquipe MCP  
**Statut**: APPROVED

**Prochaine r√©vision**: Fin de chaque phase  
**Approbation**: Product Owner + Tech Lead

---

*Ce document est un plan vivant et sera mis √† jour au fur et √† mesure de l'avancement du projet.*

