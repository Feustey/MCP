version: '3.8'

services:
  # ============================================
  # REVERSE PROXY PRINCIPAL
  # ============================================
  nginx:
    image: nginx:alpine
    container_name: hostinger-nginx
    restart: unless-stopped
    ports:
      - "80:80"
      - "443:443"
      - "127.0.0.1:8080:8080"  # Monitoring interne
    volumes:
      - ./config/nginx/hostinger-unified.conf:/etc/nginx/conf.d/default.conf:ro
      - ./config/nginx/nginx.conf:/etc/nginx/nginx.conf:ro
      - /etc/ssl/certs:/etc/ssl/certs:ro
      - /etc/ssl/private:/etc/ssl/private:ro
      - ./logs/nginx:/var/log/nginx
      - ./config/nginx/.htpasswd:/etc/nginx/.htpasswd:ro
    depends_on:
      - mcp-api
      - t4g-api
    networks:
      - hostinger-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ============================================
  # MCP API PRINCIPAL avec RAG
  # ============================================
  mcp-api:
    image: feustey/dazno:latest
    container_name: mcp-api
    restart: unless-stopped
    expose:
      - "8000"
    environment:
      # Configuration serveur
      - ENVIRONMENT=production
      - DEBUG=false
      - HOST=0.0.0.0
      - PORT=8000
      - WORKERS=4
      - RELOAD=false
      
      # Base de données
      - MONGO_URL=mongodb+srv://feustey:sIiEp8oiB2hjYBbi@dazia.pin4fwl.mongodb.net/mcp?retryWrites=true&w=majority&appName=Dazia
      - REDIS_URL=redis://default:EqbM5xJAkh9gvdOyVoYiWR9EoHRBXcjY@redis-16818.crce202.eu-west-3-1.ec2.redns.redis-cloud.com:16818/0
      
      # Sécurité et JWT
      - JWT_SECRET_KEY=${JWT_SECRET_KEY}
      - SECRET_KEY=${SECRET_KEY}
      - CORS_ORIGINS=https://app.dazno.de,https://api.dazno.de,https://token-for-good.com
      
      # Services Lightning
      - LIGHTNING_ADDRESS=feustey@getalby.com
      - LNBITS_URL=https://lnbits.dazno.de
      - LNBITS_ADMIN_KEY=${LNBITS_ADMIN_KEY}
      - LNBITS_INKEY=${LNBITS_INKEY}
      
      # Intelligence Artificielle
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      
      # Configuration RAG
      - RAG_ENABLED=true
      - RAG_DATA_PATH=/app/rag
      - QDRANT_URL=http://qdrant:6333
      - QDRANT_COLLECTION=mcp_knowledge
      - MODEL_NAME=gpt-4o-mini
      - EMBEDDING_MODEL=text-embedding-ada-002
      
      # APIs externes
      - SPARKSEER_API_KEY=${SPARKSEER_API_KEY}
      - SPARKSEER_BASE_URL=https://api.sparkseer.space/v1/
      
      # Notifications
      - TELEGRAM_BOT_TOKEN=${TELEGRAM_BOT_TOKEN}
      - TELEGRAM_CHAT_ID=${TELEGRAM_CHAT_ID}
      
      # Performance et Rate Limiting
      - RATE_LIMIT_ENABLED=true
      - MAX_REQUESTS_PER_HOUR=1000
      - BATCH_SIZE=10
      - MAX_CONCURRENT_REQUESTS=10
      - REQUEST_TIMEOUT=60
      
      # Logging
      - LOG_LEVEL=INFO
      - LOG_FORMAT=json
      - LOG_FILE=/app/logs/mcp.log
      
    volumes:
      - ./mcp-data/logs:/app/logs
      - ./mcp-data/data:/app/data
      - ./mcp-data/rag:/app/rag
      - ./mcp-data/backups:/app/backups
      - ./mcp-data/reports:/app/reports
      
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/api/v1/health"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s
      
    networks:
      - hostinger-network
    depends_on:
      - qdrant

  # ============================================
  # TOKEN-FOR-GOOD API
  # ============================================
  t4g-api:
    image: feustey/token-for-good:latest
    container_name: t4g-api
    restart: unless-stopped
    expose:
      - "8001"
    environment:
      - NODE_ENV=production
      - PORT=8001
      - MONGO_URL=mongodb+srv://feustey:sIiEp8oiB2hjYBbi@dazia.pin4fwl.mongodb.net/t4g?retryWrites=true&w=majority&appName=Dazia
      - REDIS_URL=redis://default:EqbM5xJAkh9gvdOyVoYiWR9EoHRBXcjY@redis-16818.crce202.eu-west-3-1.ec2.redns.redis-cloud.com:16818/1
      - JWT_SECRET=${T4G_JWT_SECRET}
      - API_KEY=${T4G_API_KEY}
      - CORS_ORIGINS=https://app.dazno.de,https://token-for-good.com
      - SUPABASE_URL=${SUPABASE_URL}
      - SUPABASE_ROLE=${SUPABASE_ROLE}
    volumes:
      - ./t4g-data/logs:/app/logs
      - ./t4g-data/uploads:/app/uploads
      - ./t4g-data/data:/app/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8001/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - hostinger-network

  # ============================================
  # QDRANT VECTOR DATABASE (RAG)
  # ============================================
  qdrant:
    image: qdrant/qdrant:v1.7.4
    container_name: hostinger-qdrant
    restart: unless-stopped
    expose:
      - "6333"
      - "6334"
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
    volumes:
      - qdrant_data:/qdrant/storage
      - ./config/qdrant:/qdrant/config:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - hostinger-network


  # ============================================
  # MONITORING - PROMETHEUS
  # ============================================
  prometheus:
    image: prom/prometheus:v2.47.2
    container_name: hostinger-prometheus
    restart: unless-stopped
    expose:
      - "9090"
    volumes:
      - ./config/prometheus/prometheus-unified.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=15d'
      - '--storage.tsdb.retention.size=10GB'
      - '--web.enable-lifecycle'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - hostinger-network

  # ============================================
  # MONITORING - GRAFANA
  # ============================================
  grafana:
    image: grafana/grafana:10.2.0
    container_name: hostinger-grafana
    restart: unless-stopped
    expose:
      - "3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_SERVER_ROOT_URL=%(protocol)s://%(domain)s:%(http_port)s/grafana/
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_ANALYTICS_REPORTING_ENABLED=false
      - GF_ANALYTICS_CHECK_FOR_UPDATES=false
      - GF_INSTALL_PLUGINS=grafana-clock-panel,grafana-simple-json-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./config/grafana/dashboards:/var/lib/grafana/dashboards:ro
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    networks:
      - hostinger-network
    depends_on:
      - prometheus

  # ============================================
  # BACKUP SERVICE
  # ============================================
  backup:
    image: alpine:3.18
    container_name: hostinger-backup
    restart: "no"
    volumes:
      - ./mcp-data:/backup/mcp-data:ro
      - ./t4g-data:/backup/t4g-data:ro
      - ./backups:/backup/output
      - ./scripts:/scripts:ro
    networks:
      - hostinger-network
    command: >
      sh -c "
        apk add --no-cache curl tar gzip &&
        echo 'Backup service ready' &&
        tail -f /dev/null
      "

volumes:
  qdrant_data:
    driver: local
  prometheus_data:
    driver: local
  grafana_data:
    driver: local

networks:
  hostinger-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16
    driver_opts:
      com.docker.network.bridge.name: mcp-bridge
