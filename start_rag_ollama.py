#!/usr/bin/env python3
"""
Script pour lancer le syst√®me RAG MCP avec Ollama en local
Derni√®re mise √† jour: 25 mai 2025
"""

import os
import sys
import asyncio
import logging
from pathlib import Path
import uvicorn
from prometheus_client import start_http_server
from config.rag_config import settings

# Ajouter le r√©pertoire du projet au PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configuration du logging
logging.basicConfig(
    level=getattr(logging, settings.LOG_LEVEL),
    format=settings.LOG_FORMAT
)
logger = logging.getLogger(__name__)

async def check_dependencies():
    """V√©rifie que toutes les d√©pendances sont disponibles."""
    try:
        import redis
        import motor
        import prometheus_client
        return True
    except ImportError as e:
        logger.error(f"D√©pendance manquante: {e}")
        return False

async def start_metrics_server():
    """D√©marre le serveur de m√©triques Prometheus."""
    if settings.ENABLE_METRICS:
        try:
            start_http_server(settings.METRICS_PORT)
            logger.info(f"Serveur de m√©triques d√©marr√© sur le port {settings.METRICS_PORT}")
        except Exception as e:
            logger.error(f"Erreur lors du d√©marrage du serveur de m√©triques: {e}")

async def check_services():
    """V√©rifie que tous les services n√©cessaires sont disponibles."""
    from rag.ollama_client import OllamaClient
    from rag.cache import RedisCache
    
    # V√©rifier Ollama
    ollama_client = OllamaClient()
    ollama_status = await ollama_client.test_connection()
    if not ollama_status:
        logger.error("Ollama n'est pas accessible")
        return False
    
    # V√©rifier Redis
    try:
        cache = RedisCache()
        await cache.redis.ping()
        logger.info("Redis est accessible")
    except Exception as e:
        logger.error(f"Redis n'est pas accessible: {e}")
        return False
    
    return True

async def main():
    """Fonction principale."""
    print("üöÄ D√©marrage du syst√®me RAG MCP avec Ollama")
    print("=" * 50)
    
    # V√©rifier les d√©pendances
    if not await check_dependencies():
        print("‚ùå D√©pendances manquantes")
        return 1
    
    # V√©rifier les services
    if not await check_services():
        print("‚ùå Services non disponibles")
        return 1
    
    # D√©marrer le serveur de m√©triques
    await start_metrics_server()
    
    # D√©marrer l'API
    print("‚úÖ Tous les services sont pr√™ts")
    print("üåê API disponible sur: http://localhost:8000")
    print("üìä M√©triques disponibles sur: http://localhost:9090")
    print("üìö Documentation disponible sur: http://localhost:8000/docs")
    
    uvicorn.run(
        "rag_api:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level=settings.LOG_LEVEL.lower()
    )
    
    return 0

if __name__ == "__main__":
    exit(asyncio.run(main())) 