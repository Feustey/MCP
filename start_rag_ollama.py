#!/usr/bin/env python3
"""
Script pour lancer le syst√®me RAG MCP avec Ollama en local
Derni√®re mise √† jour: 25 mai 2025
"""

import os
import sys
import asyncio
import logging
from pathlib import Path

# Ajouter le r√©pertoire du projet au PYTHONPATH
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Importer les modules n√©cessaires
try:
    from rag.rag import RAGWorkflow, OllamaEmbedder
    from rag.mongo_operations import MongoOperations
except ImportError as e:
    logger.error(f"Erreur d'import: {e}")
    logger.info("Utilisation d'une version simplifi√©e sans d√©pendances probl√©matiques")
    
    # Version simplifi√©e pour contourner les probl√®mes d'import
    import httpx
    import json
    
    class SimpleOllamaClient:
        def __init__(self, base_url="http://localhost:11434"):
            self.base_url = base_url
            
        async def get_embedding(self, text: str):
            """Obtient un embedding via Ollama."""
            try:
                url = f"{self.base_url}/api/embeddings"
                data = {
                    "model": "llama3",
                    "prompt": text
                }
                
                async with httpx.AsyncClient(timeout=30.0) as client:
                    response = await client.post(url, json=data)
                    response.raise_for_status()
                    result = response.json()
                    return result.get("embedding", [])
            except Exception as e:
                logger.error(f"Erreur embedding: {e}")
                return []
        
        async def generate_response(self, prompt: str, model="llama3"):
            """G√©n√®re une r√©ponse via Ollama."""
            try:
                url = f"{self.base_url}/api/generate"
                data = {
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                }
                
                async with httpx.AsyncClient(timeout=60.0) as client:
                    response = await client.post(url, json=data)
                    response.raise_for_status()
                    result = response.json()
                    return result.get("response", "")
            except Exception as e:
                logger.error(f"Erreur g√©n√©ration: {e}")
                return f"Erreur: {e}"


async def test_ollama_connection():
    """Test la connexion √† Ollama."""
    try:
        async with httpx.AsyncClient(timeout=10.0) as client:
            response = await client.get("http://localhost:11434/api/tags")
            response.raise_for_status()
            models = response.json()
            logger.info(f"Ollama accessible. Mod√®les disponibles: {[m['name'] for m in models['models']]}")
            return True
    except Exception as e:
        logger.error(f"Ollama non accessible: {e}")
        return False


async def run_simple_rag_test():
    """Lance un test simple du syst√®me RAG avec Ollama."""
    
    # Test de connexion Ollama
    if not await test_ollama_connection():
        logger.error("Impossible de se connecter √† Ollama. Assurez-vous qu'il est lanc√©.")
        return False
    
    # Cr√©er le client Ollama
    client = SimpleOllamaClient()
    
    # Test d'embedding
    logger.info("Test de g√©n√©ration d'embedding...")
    test_text = "Configuration des frais Lightning Network pour optimiser les revenus"
    embedding = await client.get_embedding(test_text)
    
    if embedding:
        logger.info(f"Embedding g√©n√©r√© avec succ√®s (dimension: {len(embedding)})")
    else:
        logger.error("√âchec de g√©n√©ration d'embedding")
        return False
    
    # Test de g√©n√©ration de r√©ponse
    logger.info("Test de g√©n√©ration de r√©ponse...")
    prompt = """En tant qu'expert Lightning Network, analysez cette situation:

Un n≈ìud Lightning a les m√©triques suivantes:
- Capacit√© totale: 5 BTC
- Liquidit√© sortante: 2.5 BTC  
- Liquidit√© entrante: 2.5 BTC
- Frais base: 1000 msat
- Frais proportionnels: 100 ppm

Question: Quelles recommandations donneriez-vous pour optimiser les revenus de ce n≈ìud?

R√©pondez de mani√®re concise et pratique."""

    response = await client.generate_response(prompt)
    
    if response and "Erreur:" not in response:
        logger.info("R√©ponse g√©n√©r√©e avec succ√®s:")
        logger.info(f"R√©ponse: {response[:300]}...")
        return True
    else:
        logger.error(f"√âchec de g√©n√©ration de r√©ponse: {response}")
        return False


async def run_full_rag_workflow():
    """Lance le workflow RAG complet si les d√©pendances sont disponibles."""
    try:
        # Cr√©er les r√©pertoires n√©cessaires
        os.makedirs("rag/RAG_assets/reports", exist_ok=True)
        os.makedirs("data/raw", exist_ok=True)
        
        # Initialiser le workflow RAG
        logger.info("Initialisation du workflow RAG complet...")
        rag = RAGWorkflow()
        
        # Test d'une requ√™te simple
        query = "Quelles sont les meilleures pratiques pour optimiser les frais d'un n≈ìud Lightning?"
        logger.info(f"Test de requ√™te: {query}")
        
        response = await rag.query(query)
        logger.info(f"R√©ponse RAG: {response}")
        
        return True
    except Exception as e:
        logger.error(f"Erreur dans le workflow RAG complet: {e}")
        return False


async def main():
    """Fonction principale."""
    print("üöÄ Lancement du syst√®me RAG MCP avec Ollama")
    print("=" * 50)
    
    # Test simple d'abord
    logger.info("Phase 1: Test simple avec Ollama")
    simple_success = await run_simple_rag_test()
    
    if simple_success:
        print("‚úÖ Test simple Ollama r√©ussi")
        
        # Tentative du workflow complet
        logger.info("Phase 2: Tentative du workflow RAG complet")
        try:
            full_success = await run_full_rag_workflow()
            if full_success:
                print("‚úÖ Workflow RAG complet r√©ussi")
            else:
                print("‚ö†Ô∏è  Workflow complet √©chou√©, mais test simple OK")
        except Exception as e:
            logger.info(f"Workflow complet non disponible: {e}")
            print("‚ö†Ô∏è  Workflow complet non disponible, mais test simple fonctionne")
    else:
        print("‚ùå Test simple Ollama √©chou√©")
        return 1
    
    print("\nüéØ Syst√®me RAG op√©rationnel avec Ollama!")
    print("üìç Pour tester manuellement:")
    print("   curl -X POST http://localhost:11434/api/generate -d '{\"model\":\"llama3\",\"prompt\":\"Test\",\"stream\":false}'")
    
    return 0


if __name__ == "__main__":
    exit(asyncio.run(main())) 