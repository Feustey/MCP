#!/usr/bin/env python3
"""
API RAG simple pour MCP avec Ollama
Derni√®re mise √† jour: 25 mai 2025
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import httpx
import asyncio
import uvicorn
from datetime import datetime
import logging

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Mod√®les Pydantic
class QueryRequest(BaseModel):
    query: str
    model: str = "llama3"
    max_length: int = 500

class EmbeddingRequest(BaseModel):
    text: str
    model: str = "llama3"

class AnalysisRequest(BaseModel):
    node_data: dict
    analysis_type: str = "fee_optimization"

# Cr√©ation de l'app FastAPI
app = FastAPI(
    title="MCP RAG API avec Ollama",
    description="API pour le syst√®me RAG Lightning Network avec Ollama",
    version="1.0.0"
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class OllamaClient:
    """Client simple pour Ollama."""
    
    def __init__(self, base_url="http://localhost:11434"):
        self.base_url = base_url
    
    async def test_connection(self):
        """Test la connexion √† Ollama."""
        try:
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                return response.status_code == 200
        except:
            return False
    
    async def get_models(self):
        """R√©cup√®re la liste des mod√®les disponibles."""
        try:
            async with httpx.AsyncClient(timeout=10.0) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                response.raise_for_status()
                return response.json()
        except Exception as e:
            logger.error(f"Erreur r√©cup√©ration mod√®les: {e}")
            return {"models": []}
    
    async def generate_response(self, prompt: str, model="llama3"):
        """G√©n√®re une r√©ponse via Ollama."""
        try:
            url = f"{self.base_url}/api/generate"
            data = {
                "model": model,
                "prompt": prompt,
                "stream": False
            }
            
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(url, json=data)
                response.raise_for_status()
                result = response.json()
                return result.get("response", "")
        except Exception as e:
            logger.error(f"Erreur g√©n√©ration: {e}")
            raise HTTPException(status_code=500, detail=f"Erreur g√©n√©ration: {e}")
    
    async def get_embedding(self, text: str, model="llama3"):
        """G√©n√®re un embedding via Ollama."""
        try:
            url = f"{self.base_url}/api/embeddings"
            data = {
                "model": model,
                "prompt": text
            }
            
            async with httpx.AsyncClient(timeout=60.0) as client:
                response = await client.post(url, json=data)
                response.raise_for_status()
                result = response.json()
                return result.get("embedding", [])
        except Exception as e:
            logger.error(f"Erreur embedding: {e}")
            raise HTTPException(status_code=500, detail=f"Erreur embedding: {e}")

# Instance globale du client Ollama
ollama_client = OllamaClient()

@app.get("/")
async def root():
    """Page d'accueil de l'API RAG."""
    return {
        "service": "MCP RAG API avec Ollama",
        "status": "running",
        "timestamp": datetime.now().isoformat(),
        "endpoints": [
            "/health",
            "/models",
            "/query",
            "/embed",
            "/analyze",
            "/lightning/optimize",
            "/docs"
        ]
    }

@app.get("/health")
async def health_check():
    """V√©rification de sant√© incluant Ollama."""
    ollama_status = await ollama_client.test_connection()
    
    return {
        "status": "healthy" if ollama_status else "degraded",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "api": "running",
            "ollama": "connected" if ollama_status else "disconnected"
        },
        "ollama_url": ollama_client.base_url
    }

@app.get("/models")
async def get_available_models():
    """R√©cup√®re les mod√®les disponibles dans Ollama."""
    models = await ollama_client.get_models()
    return {
        "available_models": models.get("models", []),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/query")
async def rag_query(request: QueryRequest):
    """Effectue une requ√™te RAG avec contexte Lightning Network."""
    
    # Construire le prompt avec contexte Lightning Network
    lightning_context = """
Tu es un expert en Lightning Network et en optimisation de n≈ìuds. Tu dois r√©pondre aux questions 
avec une expertise technique approfondie sur:
- Configuration des frais (base fee, fee rate)
- Gestion de la liquidit√© (inbound/outbound)
- Routing et pathfinding
- Monitoring et m√©triques
- Strat√©gies d'optimisation des revenus
- S√©curit√© et best practices

Contexte Lightning Network:
- Les n≈ìuds g√©n√®rent des revenus en routant des paiements
- Les frais doivent √™tre √©quilibr√©s (ni trop hauts, ni trop bas)
- La liquidit√© doit √™tre bien r√©partie pour maximiser les opportunit√©s de routing
- Le monitoring est crucial pour d√©tecter les probl√®mes rapidement
"""
    
    full_prompt = f"{lightning_context}\n\nQuestion: {request.query}\n\nR√©ponse:"
    
    response = await ollama_client.generate_response(full_prompt, request.model)
    
    return {
        "query": request.query,
        "response": response,
        "model": request.model,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/embed")
async def generate_embedding(request: EmbeddingRequest):
    """G√©n√®re un embedding pour un texte."""
    embedding = await ollama_client.get_embedding(request.text, request.model)
    
    return {
        "text": request.text,
        "embedding": embedding,
        "dimension": len(embedding),
        "model": request.model,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/analyze")
async def analyze_data(request: AnalysisRequest):
    """Analyse des donn√©es Lightning Network."""
    
    node_data = request.node_data
    analysis_type = request.analysis_type
    
    # Construire le prompt d'analyse
    if analysis_type == "fee_optimization":
        prompt = f"""
Analyse les donn√©es suivantes d'un n≈ìud Lightning Network et fournis des recommandations d'optimisation des frais:

Donn√©es du n≈ìud:
{node_data}

Fournis une analyse structur√©e avec:
1. √âtat actuel du n≈ìud
2. Points d'am√©lioration identifi√©s  
3. Recommandations concr√®tes pour les frais
4. Strat√©gies de liquidit√©
5. Actions prioritaires

Sois pr√©cis et actionnable dans tes recommandations.
"""
    else:
        prompt = f"""
Analyse g√©n√©rale des donn√©es Lightning Network:

Donn√©es:
{node_data}

Fournis une analyse compl√®te avec insights et recommandations.
"""
    
    analysis = await ollama_client.generate_response(prompt)
    
    return {
        "node_data": node_data,
        "analysis_type": analysis_type,
        "analysis": analysis,
        "timestamp": datetime.now().isoformat()
    }

@app.post("/lightning/optimize")
async def optimize_lightning_node(node_data: dict):
    """Optimisation sp√©cifique pour n≈ìuds Lightning."""
    
    prompt = f"""
En tant qu'expert Lightning Network, analyse ce n≈ìud et fournis des recommandations d'optimisation:

DONN√âES DU N≈íUD:
{node_data}

ANALYSE DEMAND√âE:
1. Configuration des frais actuelle
2. √âtat de la liquidit√©
3. Performance de routing
4. Recommandations d'am√©lioration
5. Actions imm√©diates √† prendre

Fournis des recommandations CONCR√àTES et ACTIONNABLES avec des valeurs num√©riques pr√©cises.
"""
    
    optimization = await ollama_client.generate_response(prompt)
    
    return {
        "node_data": node_data,
        "optimization_recommendations": optimization,
        "timestamp": datetime.now().isoformat(),
        "expert_system": "MCP RAG Lightning Optimizer"
    }

@app.get("/lightning/health/{node_id}")
async def node_health_check(node_id: str):
    """V√©rification de sant√© d'un n≈ìud Lightning."""
    
    # Simuler une v√©rification de sant√©
    prompt = f"""
Effectue une v√©rification de sant√© pour le n≈ìud Lightning {node_id}.

V√©rifie:
1. Connectivit√© r√©seau
2. √âtat des channels
3. Liquidit√© disponible
4. Performance r√©cente
5. Alertes potentielles

Fournis un rapport de sant√© structur√©.
"""
    
    health_report = await ollama_client.generate_response(prompt)
    
    return {
        "node_id": node_id,
        "health_status": "checked",
        "report": health_report,
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    print("üöÄ Lancement de l'API RAG MCP avec Ollama...")
    print("üìç Disponible sur: http://localhost:8001")
    print("üìñ Documentation: http://localhost:8001/docs")
    print("üîç Sant√©: http://localhost:8001/health")
    print("üß† Mod√®les: http://localhost:8001/models")
    
    uvicorn.run(app, host="0.0.0.0", port=8001, log_level="info") 