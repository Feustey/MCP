#!/bin/bash

# Script pour t√©l√©charger et configurer les mod√®les Ollama optimaux pour MCP
# Usage: ./scripts/setup_ollama_models.sh [profile]
# Profiles: minimal, recommended, full

set -e

# Couleurs pour output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

echo -e "${BLUE}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
echo -e "${BLUE}‚ïë    MCP Ollama Models Setup - Lightning Optimizer        ‚ïë${NC}"
echo -e "${BLUE}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
echo ""

# V√©rifier qu'Ollama est install√©
if ! command -v ollama &> /dev/null; then
    echo -e "${RED}‚ùå Ollama n'est pas install√©${NC}"
    echo ""
    echo "Installation:"
    echo "  macOS/Linux: curl -fsSL https://ollama.com/install.sh | sh"
    echo "  Windows: https://ollama.com/download"
    echo ""
    exit 1
fi

echo -e "${GREEN}‚úì Ollama est install√©${NC}"
echo ""

# V√©rifier qu'Ollama est lanc√©
if ! ollama list &> /dev/null; then
    echo -e "${YELLOW}‚ö†Ô∏è  Ollama n'est pas lanc√©${NC}"
    echo "D√©marrage d'Ollama..."
    ollama serve &
    sleep 3
fi

# D√©terminer le profil
PROFILE=${1:-recommended}

echo -e "${BLUE}Profil s√©lectionn√©: ${PROFILE}${NC}"
echo ""

# D√©finir les mod√®les selon le profil
case $PROFILE in
    minimal)
        echo "Profil MINIMAL: Mod√®les l√©gers pour RAM limit√©e (< 16GB)"
        MODELS=(
            "llama3:8b-instruct"
            "phi3:medium"
        )
        ;;
    
    recommended)
        echo "Profil RECOMMAND√â: Balance performance/qualit√© (16-32GB RAM)"
        MODELS=(
            "llama3:8b-instruct"
            "phi3:medium"
            "qwen2.5:14b-instruct"
            "codellama:13b-instruct"
        )
        ;;
    
    full)
        echo "Profil FULL: Tous les mod√®les optimis√©s (32GB+ RAM)"
        MODELS=(
            "llama3:8b-instruct"
            "llama3:13b-instruct"
            "phi3:medium"
            "qwen2.5:14b-instruct"
            "codellama:13b-instruct"
            "mistral:7b-instruct"
        )
        ;;
    
    *)
        echo -e "${RED}Profil inconnu: $PROFILE${NC}"
        echo "Profils disponibles: minimal, recommended, full"
        exit 1
        ;;
esac

echo ""
echo -e "${GREEN}Mod√®les √† t√©l√©charger: ${#MODELS[@]}${NC}"
for model in "${MODELS[@]}"; do
    echo "  - $model"
done
echo ""

# Fonction pour t√©l√©charger un mod√®le
download_model() {
    local model=$1
    echo -e "${BLUE}üì• T√©l√©chargement de ${model}...${NC}"
    
    if ollama pull $model; then
        echo -e "${GREEN}‚úì ${model} t√©l√©charg√© avec succ√®s${NC}"
        return 0
    else
        echo -e "${RED}‚úó √âchec du t√©l√©chargement de ${model}${NC}"
        return 1
    fi
}

# T√©l√©charger chaque mod√®le
SUCCESS_COUNT=0
FAIL_COUNT=0

for model in "${MODELS[@]}"; do
    # V√©rifier si d√©j√† t√©l√©charg√©
    if ollama list | grep -q "^$model"; then
        echo -e "${YELLOW}‚è≠  ${model} d√©j√† pr√©sent, skip${NC}"
        SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
    else
        if download_model $model; then
            SUCCESS_COUNT=$((SUCCESS_COUNT + 1))
        else
            FAIL_COUNT=$((FAIL_COUNT + 1))
        fi
    fi
    echo ""
done

# R√©sum√©
echo -e "${BLUE}‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó${NC}"
echo -e "${BLUE}‚ïë                    R√âSUM√â                                ‚ïë${NC}"
echo -e "${BLUE}‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù${NC}"
echo ""
echo -e "${GREEN}‚úì Succ√®s: ${SUCCESS_COUNT}/${#MODELS[@]}${NC}"
if [ $FAIL_COUNT -gt 0 ]; then
    echo -e "${RED}‚úó √âchecs: ${FAIL_COUNT}/${#MODELS[@]}${NC}"
fi
echo ""

# Lister les mod√®les disponibles
echo -e "${BLUE}Mod√®les Ollama install√©s:${NC}"
ollama list
echo ""

# Test rapide
echo -e "${BLUE}Test rapide du mod√®le principal...${NC}"
if echo "R√©sume Lightning Network en 2 phrases" | ollama run llama3:8b-instruct > /dev/null 2>&1; then
    echo -e "${GREEN}‚úì Mod√®le principal op√©rationnel${NC}"
else
    echo -e "${YELLOW}‚ö†Ô∏è  Test √©chou√©, mais mod√®les install√©s${NC}"
fi
echo ""

# Recommandations
echo -e "${BLUE}Prochaines √©tapes:${NC}"
echo ""
echo "1. Tester un mod√®le:"
echo "   ollama run llama3:8b-instruct"
echo ""
echo "2. V√©rifier la configuration MCP:"
echo "   cat .env | grep -E '(EMBED_MODEL|GEN_MODEL)'"
echo ""
echo "3. Lancer l'API MCP:"
echo "   uvicorn main:app --reload"
echo ""
echo "4. Tester les recommandations optimis√©es:"
echo "   python scripts/test_ollama_recommendations.py"
echo ""

# Configuration recommand√©e .env
if [ ! -f .env ]; then
    echo -e "${YELLOW}‚ö†Ô∏è  Fichier .env non trouv√©${NC}"
    echo ""
    echo "Configuration recommand√©e pour .env:"
    echo ""
    cat << EOF
# Ollama Models Configuration
EMBED_MODEL=nomic-embed-text
GEN_MODEL=qwen2.5:14b-instruct
GEN_MODEL_FALLBACK=llama3:8b-instruct

# Ollama Parameters
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_TIMEOUT=90

# RAG Configuration
RAG_TOPK=5
RAG_TEMPERATURE=0.3
RAG_MAX_TOKENS=2500
EOF
    echo ""
fi

echo -e "${GREEN}‚úì Setup Ollama termin√© !${NC}"
echo ""

