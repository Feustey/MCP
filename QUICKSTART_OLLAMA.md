# üöÄ Quick Start: Ollama/Llama 3 dans MCP

> D√©marrage rapide en 5 minutes

## 1. Configuration (`.env`)

```bash
LLM_PROVIDER=ollama
OLLAMA_URL=http://ollama:11434
GEN_MODEL=llama3:70b-instruct-2025-07-01
GEN_MODEL_FALLBACK=llama3:8b-instruct
EMBED_MODEL=nomic-embed-text
EMBED_DIMENSION=768
```

## 2. D√©marrage

```bash
# D√©marrer Ollama
docker-compose -f docker-compose.production.yml up -d ollama

# Attendre 30-60s puis initialiser les mod√®les
docker exec mcp-ollama /scripts/ollama_init.sh

# D√©marrer l'API
docker-compose -f docker-compose.production.yml up -d mcp-api
```

## 3. V√©rification

```bash
# Mod√®les install√©s
docker exec mcp-ollama ollama list

# Logs
docker logs mcp-ollama
docker logs mcp-api

# Test embedding
docker exec mcp-api python -c "
from src.clients.ollama_client import ollama_client
import asyncio
emb = asyncio.run(ollama_client.embed('test'))
print(f'‚úÖ Embedding OK: dimension={len(emb)}')
"
```

## 4. Tests

```bash
# Tests unitaires
pytest tests/unit/test_ollama_client.py -v
pytest tests/unit/test_rag_ollama_adapter.py -v
```

## üìö Documentation compl√®te

- **[Guide d'int√©gration](docs/OLLAMA_INTEGRATION_GUIDE.md)** ‚Äî Configuration, troubleshooting, performance
- **[Sp√©cification technique](docs/core/spec-rag-ollama.md)** ‚Äî Architecture, flux RAG, API
- **[R√©sum√© complet](OLLAMA_INTEGRATION_COMPLETE.md)** ‚Äî Vue d'ensemble de l'int√©gration

## üÜò Probl√®mes courants

### Ollama ne d√©marre pas
```bash
docker logs mcp-ollama
docker restart mcp-ollama
```

### Mod√®le non trouv√© (404)
```bash
docker exec mcp-ollama ollama pull llama3:70b-instruct-2025-07-01
docker exec mcp-ollama ollama pull llama3:8b-instruct
docker exec mcp-ollama ollama pull nomic-embed-text
```

### Out of Memory
- Utiliser quantisation Q4_K_M
- R√©duire `num_ctx` √† 4096
- Basculer sur mod√®le 8B seulement

---

**Statut:** ‚úÖ Production Ready  
**Date:** 16 octobre 2025

